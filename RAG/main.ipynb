{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e8303d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ì„¤ì • ë° ê²½ë¡œ\n",
    "PATHS = [\n",
    "    (\"SKT\", r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\SKTìš”ê¸ˆì œì •ë³´_251209.docx\"),\n",
    "    (\"KT\", r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\KTìš”ê¸ˆì œì •ë³´_251209.docx\"),\n",
    "    (\"LGU+\", r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\LGU+ìš”ê¸ˆì œì •ë³´_260102.docx\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f43dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ í†µì‹ ì‚¬ë³„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹œì‘...\n",
      "   -> SKT: 152ê°œì˜ ë…ë¦½ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ\n",
      "   -> KT: 113ê°œì˜ ë…ë¦½ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ\n",
      "   -> LGU+: 88ê°œì˜ ë…ë¦½ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 2. í…ìŠ¤íŠ¸ ë¶„ë¦¬(Parsing) ë¡œì§\n",
    "carrier_plans = {\"SKT\": [], \"LGU+\": [], \"KT\": []}\n",
    "\n",
    "print(\"ğŸ“„ í†µì‹ ì‚¬ë³„ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "for carrier_name, path in PATHS:\n",
    "    doc = Document(path)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    current_block = []\n",
    "    \n",
    "    for para in paras:\n",
    "        is_new_section = False\n",
    "        \n",
    "        # ì¡°ê±´ 1: \"ìš”ê¸ˆì œë€?\" í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ë‹¨ (ì„¤ëª… ì„¹ì…˜ì˜ ì‹œì‘)\n",
    "        if \"ìš”ê¸ˆì œë€?\" in para:\n",
    "            is_new_section = True\n",
    "            \n",
    "        # ì¡°ê±´ 2: \"[í†µì‹ ì‚¬ëª…]\" íƒœê·¸ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ë‹¨ (ìƒì„¸ ì •ë³´ ì„¹ì…˜ì˜ ì‹œì‘)\n",
    "        elif para.startswith(f\"[{carrier_name}]\"):\n",
    "            is_new_section = True\n",
    "        \n",
    "        # LGU+ì˜ ê²½ìš° ë¬¸ì„œ í‘œê¸°ì— ë”°ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "        elif carrier_name == \"LGU+\" and para.startswith(\"[LG U+\"):\n",
    "            is_new_section = True\n",
    "\n",
    "        # ìƒˆë¡œìš´ ì„¹ì…˜ì´ ê°ì§€ë˜ë©´ ì´ì „ê¹Œì§€ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ ì €ì¥\n",
    "        if is_new_section and current_block:\n",
    "            carrier_plans[carrier_name].append(\"\\n\".join(current_block))\n",
    "            current_block = []\n",
    "        \n",
    "        current_block.append(para)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë‚¨ì€ ë¸”ë¡ ì €ì¥\n",
    "    if current_block:\n",
    "        carrier_plans[carrier_name].append(\"\\n\".join(current_block))\n",
    "    \n",
    "    print(f\"   -> {carrier_name}: {len(carrier_plans[carrier_name])}ê°œì˜ ë…ë¦½ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfd23284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\.cache\\huggingface\\modules\\transformers_modules\\nomic_hyphen_ai\\nomic_hyphen_bert_hyphen_2048\\7710840340a098cfb869c4f65e87cf2b1b70caca\\modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKT] ì„ë² ë”© ê³„ì‚° ë° CSV ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:34<00:00,  9.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> SKT_plans_final.csv ì €ì¥ ì™„ë£Œ (152í–‰)\n",
      "[LGU+] ì„ë² ë”© ê³„ì‚° ë° CSV ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:41<00:00,  6.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> LGUplus_plans_final.csv ì €ì¥ ì™„ë£Œ (88í–‰)\n",
      "[KT] ì„ë² ë”© ê³„ì‚° ë° CSV ìƒì„± ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:55<00:00,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> KT_plans_final.csv ì €ì¥ ì™„ë£Œ (113í–‰)\n",
      "\n",
      "âœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! í…ìŠ¤íŠ¸ ëˆ„ë½ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”©\n",
    "print(\"\\n ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "for carrier in [\"SKT\", \"LGU+\", \"KT\"]:\n",
    "    plans = carrier_plans[carrier]\n",
    "    if not plans: continue\n",
    "        \n",
    "    print(f\"[{carrier}] ì„ë² ë”© ê³„ì‚° ë° CSV ìƒì„± ì¤‘...\")\n",
    "    embeddings = model.encode(plans, batch_size=16, normalize_embeddings=True, show_progress_bar=True)\n",
    "    \n",
    "    # 4. CSV ë°ì´í„° ìƒì„± ë° ì €ì¥\n",
    "    csv_data = []\n",
    "    pk_prefix = \"LGU\" if carrier == \"LGU+\" else carrier\n",
    "    \n",
    "    for i, (full_text, embed) in enumerate(zip(plans, embeddings)):\n",
    "        # ë°ì´í„° ì„±ê²© íŒë³„ (INFO ì¸ì§€ CONT ì¸ì§€)\n",
    "        is_info = True if carrier in full_text and (\"í†µì‹ ì‚¬ :\" in full_text or \"ê°€ê²© :\" in full_text) else False\n",
    "        suffix = \"INFO\" if is_info else \"CONT\"\n",
    "        \n",
    "        csv_data.append({\n",
    "            \"dense\": json.dumps(embed.tolist()),\n",
    "            \"source\": carrier,\n",
    "            \"file_hash\": f\"{carrier}_251209_final\",\n",
    "            \"page\": i + 1,\n",
    "            \"row\": 1,\n",
    "            \"text\": full_text,  # ìš”ê¸ˆì œë€? ì„¹ì…˜ í˜¹ì€ [KT] ìƒì„¸ ì„¹ì…˜ ë‚´ìš© ì „ì²´ê°€ ëˆ„ë½ ì—†ì´ ë“¤ì–´ê°\n",
    "            \"pk\": f\"{pk_prefix}{i + 1}_{suffix}\"\n",
    "        })\n",
    "    \n",
    "    # í†µì‹ ì‚¬ë³„ ê°œë³„ íŒŒì¼ ì €ì¥\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    safe_name = carrier.replace(\"+\", \"plus\")\n",
    "    df.to_csv(f\"{safe_name}_plans_final.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"   -> {safe_name}_plans_final.csv ì €ì¥ ì™„ë£Œ ({len(df)}í–‰)\")\n",
    "\n",
    "print(\"\\nâœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! í…ìŠ¤íŠ¸ ëˆ„ë½ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db93a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\.cache\\huggingface\\modules\\transformers_modules\\nomic_hyphen_ai\\nomic_hyphen_bert_hyphen_2048\\7710840340a098cfb869c4f65e87cf2b1b70caca\\modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1da04d",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ íŒŒì¼ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dde4515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "[SKT] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\n",
      "âœ… SKT_plans_final.txt ì €ì¥ ì™„ë£Œ! (ì´ 152ê°œ ì„¹ì…˜)\n",
      "[KT] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\n",
      "âœ… KT_plans_final.txt ì €ì¥ ì™„ë£Œ! (ì´ 113ê°œ ì„¹ì…˜)\n",
      "[LGU+] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\n",
      "âœ… LGUplus_plans_final.txt ì €ì¥ ì™„ë£Œ! (ì´ 88ê°œ ì„¹ì…˜)\n",
      "\n",
      "âœ¨ ëª¨ë“  í†µì‹ ì‚¬ .txt(JSONL) íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 2. Markdown ë³€í™˜ í•¨ìˆ˜ (ê³„ì¸µ êµ¬ì¡° ì ìš©)\n",
    "def convert_to_markdown(para_list, carrier_name):\n",
    "    if not para_list: return \"\"\n",
    "    \n",
    "    # [ìƒì„¸ ì •ë³´] ì„¹ì…˜ (INFO)\n",
    "    if para_list[0].startswith(f\"[{carrier_name}]\") or \"[LG U+\" in para_list[0]:\n",
    "        md_lines = [f\"# {para_list[0]}\"]\n",
    "        in_features = False\n",
    "        for line in para_list[1:]:\n",
    "            if \"ìš”ê¸ˆì œ íŠ¹ì§•\" in line:\n",
    "                md_lines.append(f\"## {line}\")\n",
    "                in_features = True\n",
    "            elif \":\" in line and not in_features:\n",
    "                md_lines.append(f\"## {line}\")\n",
    "            elif in_features:\n",
    "                clean = line.strip().lstrip('l ').lstrip('-').strip()\n",
    "                if clean: md_lines.append(f\"### {clean}\")\n",
    "            else:\n",
    "                md_lines.append(f\"### {line}\")\n",
    "        return \"\\n\".join(md_lines)\n",
    "    \n",
    "    # [ì„¤ëª… ì„¹ì…˜] (CONT)\n",
    "    else:\n",
    "        md_lines = []\n",
    "        for line in para_list:\n",
    "            if any(k in line for k in [\"ìš”ê¸ˆì œë€?\", \"íŠ¹ì§•\", \"ë°©ë²•\", \"ìœ ì˜ì‚¬í•­\"]):\n",
    "                md_lines.append(f\"## {line}\")\n",
    "            else:\n",
    "                md_lines.append(f\"### {line}\")\n",
    "        return \"\\n\".join(md_lines)\n",
    "\n",
    "# 3. ëª¨ë¸ ë¡œë“œ (ì„ë² ë”© ì¶”ì¶œìš©)\n",
    "print(\"ğŸ§  ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "# 4. ë°ì´í„° ì¶”ì¶œ ë° .txt ì €ì¥\n",
    "for name, path in PATHS:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ {name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        continue\n",
    "        \n",
    "    doc = Document(path)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    current_block = []\n",
    "    txt_output_lines = [] # .txt íŒŒì¼ì— ì €ì¥í•  í–‰ë“¤ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "    pk_prefix = \"LGU\" if name == \"LGU+\" else name\n",
    "    idx = 1\n",
    "\n",
    "    print(f\"[{name}] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\")\n",
    "    \n",
    "    for para in paras:\n",
    "        is_new = \"ìš”ê¸ˆì œë€?\" in para or para.startswith(f\"[{name}]\") or (name == \"LGU+\" and para.startswith(\"[LG U+\"))\n",
    "        \n",
    "        if is_new and current_block:\n",
    "            md_text = convert_to_markdown(current_block, name)\n",
    "            embed = model.encode([md_text], normalize_embeddings=True)[0]\n",
    "            suffix = \"INFO\" if md_text.startswith(\"# \") else \"CONT\"\n",
    "            \n",
    "            # Attu ì—…ë¡œë“œìš© JSON êµ¬ì¡° (í•œ ì¤„ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜)\n",
    "            entry = {\n",
    "                \"pk\": f\"{pk_prefix}{idx}_{suffix}\",\n",
    "                \"source\": name,\n",
    "                \"text\": md_text,\n",
    "                \"dense\": embed.tolist(),\n",
    "                \"page\": idx,\n",
    "                \"row\": 1\n",
    "            }\n",
    "            txt_output_lines.append(json.dumps(entry, ensure_ascii=False))\n",
    "            \n",
    "            current_block = []\n",
    "            idx += 1\n",
    "            \n",
    "        current_block.append(para)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë¸”ë¡ ì²˜ë¦¬\n",
    "    if current_block:\n",
    "        md_text = convert_to_markdown(current_block, name)\n",
    "        embed = model.encode([md_text], normalize_embeddings=True)[0]\n",
    "        suffix = \"INFO\" if md_text.startswith(\"# \") else \"CONT\"\n",
    "        entry = {\"pk\": f\"{pk_prefix}{idx}_{suffix}\", \"source\": name, \"text\": md_text, \"dense\": embed.tolist(), \"page\": idx, \"row\": 1}\n",
    "        txt_output_lines.append(json.dumps(entry, ensure_ascii=False))\n",
    "\n",
    "    # 5. ìµœì¢… .txt íŒŒì¼ ì €ì¥ (JSONL í˜•ì‹: í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON ê°ì²´)\n",
    "    safe_name = name.replace(\"+\", \"plus\")\n",
    "    output_filename = f\"{safe_name}_plans_final.txt\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(txt_output_lines))\n",
    "        \n",
    "    print(f\"âœ… {output_filename} ì €ì¥ ì™„ë£Œ! (ì´ {len(txt_output_lines)}ê°œ ì„¹ì…˜)\")\n",
    "\n",
    "print(\"\\nâœ¨ ëª¨ë“  í†µì‹ ì‚¬ .txt(JSONL) íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ed3e7",
   "metadata": {},
   "source": [
    "## json íŒŒì¼ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc3136dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "[SKT] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\n",
      "âœ… SKT_plans_final.json ì €ì¥ ì™„ë£Œ! (ì´ 152ê°œ ê°ì²´)\n",
      "[KT] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\n",
      "âœ… KT_plans_final.json ì €ì¥ ì™„ë£Œ! (ì´ 113ê°œ ê°ì²´)\n",
      "[LGU+] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\n",
      "âœ… LGUplus_plans_final.json ì €ì¥ ì™„ë£Œ! (ì´ 88ê°œ ê°ì²´)\n",
      "\n",
      "âœ¨ ëª¨ë“  í†µì‹ ì‚¬ .json íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 2. Markdown ë³€í™˜ í•¨ìˆ˜\n",
    "def convert_to_markdown(para_list, carrier_name):\n",
    "    if not para_list: return \"\"\n",
    "    # [ìƒì„¸ ì •ë³´] ì„¹ì…˜ (INFO)\n",
    "    if para_list[0].startswith(f\"[{carrier_name}]\") or \"[LG U+\" in para_list[0]:\n",
    "        md_lines = [f\"# {para_list[0]}\"]\n",
    "        in_features = False\n",
    "        for line in para_list[1:]:\n",
    "            if \"ìš”ê¸ˆì œ íŠ¹ì§•\" in line:\n",
    "                md_lines.append(f\"## {line}\")\n",
    "                in_features = True\n",
    "            elif \":\" in line and not in_features:\n",
    "                md_lines.append(f\"## {line}\")\n",
    "            elif in_features:\n",
    "                clean = line.strip().lstrip('l ').lstrip('-').strip()\n",
    "                if clean: md_lines.append(f\"### {clean}\")\n",
    "            else:\n",
    "                md_lines.append(f\"### {line}\")\n",
    "        return \"\\n\".join(md_lines)\n",
    "    # [ì„¤ëª… ì„¹ì…˜] (CONT)\n",
    "    else:\n",
    "        md_lines = []\n",
    "        for line in para_list:\n",
    "            if any(k in line for k in [\"ìš”ê¸ˆì œë€?\", \"íŠ¹ì§•\", \"ë°©ë²•\", \"ìœ ì˜ì‚¬í•­\"]):\n",
    "                md_lines.append(f\"## {line}\")\n",
    "            else:\n",
    "                md_lines.append(f\"### {line}\")\n",
    "        return \"\\n\".join(md_lines)\n",
    "\n",
    "# 3. ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ§  ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "# 4. ë°ì´í„° ì¶”ì¶œ ë° .json ì €ì¥\n",
    "for name, path in PATHS:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âš ï¸ {name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        continue\n",
    "        \n",
    "    doc = Document(path)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    current_block = []\n",
    "    json_output_list = [] # JSON ê°ì²´ë“¤ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "    pk_prefix = \"LGU\" if name == \"LGU+\" else name\n",
    "    idx = 1\n",
    "\n",
    "    print(f\"[{name}] ë°ì´í„° ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘...\")\n",
    "    \n",
    "    for para in paras:\n",
    "        is_new = \"ìš”ê¸ˆì œë€?\" in para or para.startswith(f\"[{name}]\") or (name == \"LGU+\" and para.startswith(\"[LG U+\"))\n",
    "        if is_new and current_block:\n",
    "            md_text = convert_to_markdown(current_block, name)\n",
    "            embed = model.encode([md_text], normalize_embeddings=True)[0]\n",
    "            suffix = \"INFO\" if md_text.startswith(\"# \") else \"CONT\"\n",
    "            \n",
    "            entry = {\n",
    "                \"pk\": f\"{pk_prefix}{idx}_{suffix}\",\n",
    "                \"source\": name,\n",
    "                \"text\": md_text,\n",
    "                \"dense\": embed.tolist(),\n",
    "                \"page\": idx,\n",
    "                \"row\": 1,\n",
    "                \"file_hash\": f\"{name}_full_page_260102\"\n",
    "            }\n",
    "            json_output_list.append(entry)\n",
    "            current_block = []\n",
    "            idx += 1\n",
    "        current_block.append(para)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë¸”ë¡ ì²˜ë¦¬\n",
    "    if current_block:\n",
    "        md_text = convert_to_markdown(current_block, name)\n",
    "        embed = model.encode([md_text], normalize_embeddings=True)[0]\n",
    "        suffix = \"INFO\" if md_text.startswith(\"# \") else \"CONT\"\n",
    "        json_output_list.append({\n",
    "            \"pk\": f\"{pk_prefix}{idx}_{suffix}\", \"source\": name, \"text\": md_text, \n",
    "            \"dense\": embed.tolist(), \"page\": idx, \"row\": 1, \"file_hash\": f\"{name}_full_page_260102\"\n",
    "        })\n",
    "\n",
    "    # 5. ìµœì¢… .json íŒŒì¼ ì €ì¥\n",
    "    safe_name = name.replace(\"+\", \"plus\")\n",
    "    output_filename = f\"{safe_name}_plans_final.json\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        # indent=4ë¥¼ í†µí•´ ê°€ë…ì„±ì´ ì¢‹ì€ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        json.dump(json_output_list, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"âœ… {output_filename} ì €ì¥ ì™„ë£Œ! (ì´ {len(json_output_list)}ê°œ ê°ì²´)\")\n",
    "\n",
    "print(\"\\nâœ¨ ëª¨ë“  í†µì‹ ì‚¬ .json íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841b7bb",
   "metadata": {},
   "source": [
    "# MD ë°©ì‹ìœ¼ë¡œ ì²­í‚¹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01e74d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ë¡œë“œ ë° ëŒ€ìš©ëŸ‰ ì²­í‚¹ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\.cache\\huggingface\\modules\\transformers_modules\\nomic_hyphen_ai\\nomic_hyphen_bert_hyphen_2048\\7710840340a098cfb869c4f65e87cf2b1b70caca\\modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì´ 113ê°œì˜ ëŒ€í˜• ì²­í¬ ìƒì„±ë¨. ì„ë² ë”© ì¤‘...\n",
      "âœ… ì„±ê³µ: C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\KT_plans_final.json ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ì„¤ì • ë° ê²½ë¡œ\n",
    "SOURCE_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\KTìš”ê¸ˆì œì •ë³´_251209.docx\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\KT_plans_final.json\"\n",
    "\n",
    "# 8,000 í† í°ì„ ìˆ˜ìš©í•˜ê¸° ìœ„í•œ ê¸€ì ìˆ˜ ì„¤ì • (í•œê¸€ ê¸°ì¤€ ì•½ 12,000 ~ 15,000ì)\n",
    "MAX_CHARS_PER_CHUNK = 500\n",
    "\n",
    "print(\"ğŸ§  ëª¨ë¸ ë¡œë“œ ë° ëŒ€ìš©ëŸ‰ ì²­í‚¹ ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def format_to_md(unit_paras):\n",
    "    \"\"\"ì‚¬ìš©ì ì§€ì • ê·œì¹™ì— ë”°ë¥¸ MD ê³„ì¸µí™”\"\"\"\n",
    "    if not unit_paras: return \"\"\n",
    "    \n",
    "    md_lines = [f\"# {unit_paras[0]}\"] # ì²« ì¤„ H1\n",
    "    section_headers = [\"OTT í˜œíƒ\", \"êµ¬ë… í˜œíƒ\", \"ë¶€ê°€ í˜œíƒ\", \"ìš”ê¸ˆì œ íŠ¹ì§•\"]\n",
    "    \n",
    "    for line in unit_paras[1:]:\n",
    "        # ê·œì¹™ 1: ì§€ì •ëœ í•µì‹¬ ì„¹ì…˜ í—¤ë”ëŠ” ##\n",
    "        if any(header in line for header in section_headers):\n",
    "            md_lines.append(f\"## {line}\")\n",
    "        # ê·œì¹™ 2: ì½œë¡ (:)ì´ í¬í•¨ëœ ì†ì„± í•­ëª©ì€ ##\n",
    "        elif \":\" in line:\n",
    "            md_lines.append(f\"## {line}\")\n",
    "        # ê·œì¹™ 3: ê·¸ ì™¸ì˜ ë‚˜ì—´ëœ íŠ¹ì§• ë° ë³¸ë¬¸ì€ ###\n",
    "        else:\n",
    "            md_lines.append(f\"### {line}\")\n",
    "    return \"\\n\".join(md_lines)\n",
    "\n",
    "def generate_rag_json():\n",
    "    if not os.path.exists(SOURCE_PATH):\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = Document(SOURCE_PATH)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # [Step 1] ìš”ê¸ˆì œ ìœ ë‹› ë¶„ë¦¬\n",
    "    units = []\n",
    "    current_unit = []\n",
    "    for para in paras:\n",
    "        if \"ìš”ê¸ˆì œë€?\" in para or para.startswith(\"[KT]\"):\n",
    "            if current_unit:\n",
    "                units.append(format_to_md(current_unit))\n",
    "                current_unit = []\n",
    "        current_unit.append(para)\n",
    "    if current_unit:\n",
    "        units.append(format_to_md(current_unit))\n",
    "\n",
    "    # [Step 2] 8,000 í† í° ê·œëª¨ë¡œ ë³‘í•©\n",
    "    final_chunks = []\n",
    "    temp_content = []\n",
    "    temp_length = 0\n",
    "\n",
    "    for md_text in units:\n",
    "        # ë³‘í•© ì‹œ ê¸€ì ìˆ˜ ê¸°ì¤€ ì²´í¬\n",
    "        if temp_length + len(md_text) > MAX_CHARS_PER_CHUNK and temp_content:\n",
    "            final_chunks.append(\"\\n\\n---\\n\\n\".join(temp_content))\n",
    "            temp_content = [md_text]\n",
    "            temp_length = len(md_text)\n",
    "        else:\n",
    "            temp_content.append(md_text)\n",
    "            temp_length += (len(md_text) + 10)\n",
    "\n",
    "    if temp_content:\n",
    "        final_chunks.append(\"\\n\\n---\\n\\n\".join(temp_content))\n",
    "\n",
    "    # [Step 3] JSON êµ¬ì„± ë° ì„ë² ë”©\n",
    "    json_data = []\n",
    "    print(f\"ğŸ“Š ì´ {len(final_chunks)}ê°œì˜ ëŒ€í˜• ì²­í¬ ìƒì„±ë¨. ì„ë² ë”© ì¤‘...\")\n",
    "    \n",
    "    for i, chunk_text in enumerate(final_chunks):\n",
    "        dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "        \n",
    "        json_data.append({\n",
    "            \"pk\": f\"KT_CHUNK_{i+1}\",\n",
    "            \"source\": \"KT\",\n",
    "            \"text\": chunk_text,\n",
    "            \"dense\": dense_vec.tolist(),\n",
    "            \"file_hash\": \"KT_260107_8K_TOKEN_ALIGNED\",\n",
    "            \"page\": i + 1,\n",
    "            \"row\": 1\n",
    "        })\n",
    "\n",
    "    # [Step 4] íŒŒì¼ ì €ì¥\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… ì„±ê³µ: {OUTPUT_PATH} ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_rag_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "778e5385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ë¡œë“œ ë° ìµœì í™” ì²­í‚¹ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\.cache\\huggingface\\modules\\transformers_modules\\nomic_hyphen_ai\\nomic_hyphen_bert_hyphen_2048\\7710840340a098cfb869c4f65e87cf2b1b70caca\\modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\KT_plans_optimized.json ì €ì¥ë¨.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ê²½ë¡œ ë° ì„¤ì •\n",
    "SOURCE_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\KTìš”ê¸ˆì œì •ë³´_251209.docx\"\n",
    "OUTPUT_JSON_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\KT_plans_optimized.json\"\n",
    "MAX_CHARS_PER_CHUNK = 2000 \n",
    "\n",
    "print(\"ğŸ§  ëª¨ë¸ ë¡œë“œ ë° ìµœì í™” ì²­í‚¹ ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def format_to_md(unit_paras):\n",
    "    \"\"\"ì§€ì •ëœ ê·œì¹™ì— ë”°ë¼ Markdown ê³„ì¸µ êµ¬ì¡° ìƒì„±\"\"\"\n",
    "    if not unit_paras: return \"\"\n",
    "    \n",
    "    md_lines = []\n",
    "    # 1. ì²« ì¤„ì€ ë¬´ì¡°ê±´ # (H1)\n",
    "    md_lines.append(f\"# {unit_paras[0]}\")\n",
    "    \n",
    "    # í•˜ìœ„ ë‚´ìš©ì„ ###ë¡œ ê°•ì œí•´ì•¼ í•˜ëŠ” 'ì„¹ì…˜ í—¤ë”' í‚¤ì›Œë“œ ì •ì˜\n",
    "    section_headers = [\"OTT í˜œíƒ\", \"êµ¬ë… í˜œíƒ\", \"ë¶€ê°€ í˜œíƒ\", \"ìš”ê¸ˆì œ íŠ¹ì§•\"]\n",
    "    \n",
    "    current_section_is_list = False # í˜„ì¬ ì„¹ì…˜ì´ ë¦¬ìŠ¤íŠ¸í˜• ì„¹ì…˜ì¸ì§€ ì¶”ì \n",
    "\n",
    "    for line in unit_paras[1:]:\n",
    "        # ê·œì¹™ A: ì„¹ì…˜ í—¤ë” í‚¤ì›Œë“œì¸ ê²½ìš° ## (H2)\n",
    "        if any(header in line for header in section_headers):\n",
    "            md_lines.append(f\"## {line}\")\n",
    "            current_section_is_list = True # ì´í›„ ë‚˜ì˜¤ëŠ” ë‚´ìš©ë“¤ì€ ###ë¡œ ì²˜ë¦¬\n",
    "        \n",
    "        # ê·œì¹™ B: ì½œë¡ (:)ì´ í¬í•¨ëœ ì¼ë°˜ ì†ì„±ì¸ ê²½ìš° ## (H2)\n",
    "        elif \":\" in line:\n",
    "            md_lines.append(f\"## {line}\")\n",
    "            current_section_is_list = False # ì†ì„±ê°’ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ ëª¨ë“œ í•´ì œ\n",
    "            \n",
    "        # ê·œì¹™ C: ê·¸ ì™¸ì˜ ì„¸ë¶€ ë‚´ìš©ì´ë‚˜ íŠ¹ì§• ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ### (H3)\n",
    "        else:\n",
    "            md_lines.append(f\"### {line}\")\n",
    "\n",
    "    return \"\\n\".join(md_lines)\n",
    "\n",
    "def generate_optimized_rag_json():\n",
    "    if not os.path.exists(SOURCE_PATH):\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = Document(SOURCE_PATH)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # [Step 1] ìš”ê¸ˆì œ ìœ ë‹› ë¶„ë¦¬\n",
    "    units = []\n",
    "    current_unit = []\n",
    "    for para in paras:\n",
    "        if \"ìš”ê¸ˆì œë€?\" in para or para.startswith(\"[KT]\"):\n",
    "            if current_unit:\n",
    "                units.append(format_to_md(current_unit))\n",
    "                current_unit = []\n",
    "        current_unit.append(para)\n",
    "    if current_unit:\n",
    "        units.append(format_to_md(current_unit))\n",
    "\n",
    "    # [Step 2] ê¸€ì ìˆ˜ ê¸°ë°˜ ë³‘í•©\n",
    "    final_chunks_text = []\n",
    "    temp_content = []\n",
    "    temp_length = 0\n",
    "\n",
    "    for md_text in units:\n",
    "        if temp_length + len(md_text) > MAX_CHARS_PER_CHUNK and temp_content:\n",
    "            final_chunks_text.append(\"\\n\\n---\\n\\n\".join(temp_content))\n",
    "            temp_content = [md_text]\n",
    "            temp_length = len(md_text)\n",
    "        else:\n",
    "            temp_content.append(md_text)\n",
    "            temp_length += len(md_text)\n",
    "\n",
    "    if temp_content:\n",
    "        final_chunks_text.append(\"\\n\\n---\\n\\n\".join(temp_content))\n",
    "\n",
    "    # [Step 3] JSON êµ¬ì„±\n",
    "    json_data = []\n",
    "    for i, chunk_text in enumerate(final_chunks_text):\n",
    "        dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "        json_data.append({\n",
    "            \"pk\": f\"KT_OPT_{i+1}\",\n",
    "            \"source\": \"KT\",\n",
    "            \"text\": chunk_text,\n",
    "            \"dense\": dense_vec.tolist(),\n",
    "            \"file_hash\": \"KT_260107_MD_FIXED\",\n",
    "            \"page\": i + 1,\n",
    "            \"row\": 1\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… ì™„ë£Œ: {OUTPUT_JSON_PATH} ì €ì¥ë¨.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_optimized_rag_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04edd17",
   "metadata": {},
   "source": [
    "# 7ê°œë¡œ ê°ê° ë‹¤ë¥¸ ì‚¬ì´ì¦ˆë¡œ ì²­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab790e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ë¡œë“œ ë° 7ê°œ ì²­í¬ ë¶„í•  ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\.cache\\huggingface\\modules\\transformers_modules\\nomic_hyphen_ai\\nomic_hyphen_bert_hyphen_2048\\7710840340a098cfb869c4f65e87cf2b1b70caca\\modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì´ 23ê°œì˜ ìš”ê¸ˆì œ ìœ ë‹› í™•ì¸ë¨. 7ê°œ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
      "ğŸ“¡ ì²­í¬ 1 ì„ë² ë”© ì¤‘... (ì•½ 31756ì)\n",
      "ğŸ“¡ ì²­í¬ 2 ì„ë² ë”© ì¤‘... (ì•½ 15660ì)\n",
      "ğŸ“¡ ì²­í¬ 3 ì„ë² ë”© ì¤‘... (ì•½ 5978ì)\n",
      "ğŸ“¡ ì²­í¬ 4 ì„ë² ë”© ì¤‘... (ì•½ 7598ì)\n",
      "ğŸ“¡ ì²­í¬ 5 ì„ë² ë”© ì¤‘... (ì•½ 6097ì)\n",
      "ğŸ“¡ ì²­í¬ 6 ì„ë² ë”© ì¤‘... (ì•½ 5890ì)\n",
      "ğŸ“¡ ì²­í¬ 7 ì„ë² ë”© ì¤‘... (ì•½ 4193ì)\n",
      "âœ… ì™„ë£Œ: ì´ 7ê°œì˜ í†µí•© ì²­í¬ê°€ C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\SKT_plans_final_7chunks.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "SOURCE_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\SKTìš”ê¸ˆì œì •ë³´_251209.docx\"\n",
    "OUTPUT_JSON_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\SKT_plans_final_7chunks.json\"\n",
    "\n",
    "print(\"ğŸ§  ëª¨ë¸ ë¡œë“œ ë° 7ê°œ ì²­í¬ ë¶„í•  ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def generate_7_chunks_json():\n",
    "    if not os.path.exists(SOURCE_PATH):\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = Document(SOURCE_PATH)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # [Step 1] ëª¨ë“  ìš”ê¸ˆì œ ì„¹ì…˜ì„ ê°œë³„ ìœ ë‹›ìœ¼ë¡œ ì¶”ì¶œ (ì´ 113ê°œ)\n",
    "    units = []\n",
    "    current_unit = []\n",
    "    for para in paras:\n",
    "        if \"ìš”ê¸ˆì œë€?\" in para or para.startswith(\"[KT]\"):\n",
    "            if current_unit:\n",
    "                units.append(current_unit)\n",
    "                current_unit = []\n",
    "        current_unit.append(para)\n",
    "    if current_unit:\n",
    "        units.append(current_unit)\n",
    "\n",
    "    print(f\"ğŸ“Š ì´ {len(units)}ê°œì˜ ìš”ê¸ˆì œ ìœ ë‹› í™•ì¸ë¨. 7ê°œ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # [Step 2] ìœ ë‹›ë“¤ì„ MD ì„œì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    md_units = []\n",
    "    for unit in units:\n",
    "        md_lines = [f\"# {unit[0]}\"] # ìš”ê¸ˆì œ ì‹œì‘ (#)\n",
    "        for line in unit[1:]:\n",
    "            if \":\" in line:\n",
    "                md_lines.append(f\"## {line}\") # ì†ì„± (##)\n",
    "            else:\n",
    "                md_lines.append(f\"### {line}\") # íŠ¹ì§•/ì„¤ëª… (###)\n",
    "        md_units.append(\"\\n\".join(md_lines))\n",
    "\n",
    "    # [Step 3] 7ê°œì˜ ê· ë“±í•œ ì²­í¬ë¡œ ê·¸ë£¹í™”\n",
    "    num_chunks = 7\n",
    "    total_units = len(md_units)\n",
    "    # ê° ì²­í¬ì— ë“¤ì–´ê°ˆ ìœ ë‹› ìˆ˜ ê³„ì‚° (ë‚˜ë¨¸ì§€ ì²˜ë¦¬ í¬í•¨)\n",
    "    units_per_chunk = total_units // num_chunks\n",
    "    remainder = total_units % num_chunks\n",
    "\n",
    "    final_chunks_text = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_chunks):\n",
    "        # ë‚˜ë¨¸ì§€ ê°œìˆ˜ë§Œí¼ ì•ì—ì„œë¶€í„° 1ê°œì”© ë” ë°°ì • (17ê°œì”© 1ë²ˆ, ë‚˜ë¨¸ì§€ëŠ” 16ê°œì”©)\n",
    "        end_idx = start_idx + units_per_chunk + (1 if i < remainder else 0)\n",
    "        chunk_group = md_units[start_idx:end_idx]\n",
    "        # êµ¬ë¶„ì(---)ë¡œ ìœ ë‹› ê²°í•©\n",
    "        final_chunks_text.append(\"\\n\\n---\\n\\n\".join(chunk_group))\n",
    "        start_idx = end_idx\n",
    "\n",
    "    # [Step 4] JSON êµ¬ì„± ë° ì„ë² ë”©\n",
    "    json_data = []\n",
    "    for i, chunk_text in enumerate(final_chunks_text):\n",
    "        print(f\"ğŸ“¡ ì²­í¬ {i+1} ì„ë² ë”© ì¤‘... (ì•½ {len(chunk_text)}ì)\")\n",
    "        dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "        \n",
    "        json_data.append({\n",
    "            \"pk\": f\"SKT_CHUNK_{i+1}\",\n",
    "            \"source\": \"SKT\",\n",
    "            \"text\": chunk_text,\n",
    "            \"dense\": dense_vec.tolist(),\n",
    "            \"file_hash\": \"SKT_260107_7CHUNKS_MD\",\n",
    "            \"page\": i + 1,\n",
    "            \"row\": 1\n",
    "        })\n",
    "\n",
    "    # [Step 5] íŒŒì¼ ì €ì¥\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… ì™„ë£Œ: ì´ {len(json_data)}ê°œì˜ í†µí•© ì²­í¬ê°€ {OUTPUT_JSON_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "generate_7_chunks_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c918dab",
   "metadata": {},
   "source": [
    "# ì²­í¬ ê· ë“±ì‚¬ì´ì¦ˆë¡œ ë¶„í• \n",
    "- SKT,KG, LGU+ ì¼€ì´ìŠ¤ë³„ë¡œ ê¸€ì ë°”ê¿€ê²ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "344d6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ëª¨ë¸ ë¡œë“œ ë° ê· ë“± ë¹„ìœ¨ ì²­í‚¹ ì‹œì‘...\n",
      "ğŸ“Š ì „ì²´ ê¸€ì ìˆ˜: 59186ì / ëª©í‘œ ì²­í¬ í¬ê¸°: ì•½ 8455ì\n",
      "ğŸ“¡ ì²­í¬ 1 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 9485ì)\n",
      "ğŸ“¡ ì²­í¬ 2 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 9184ì)\n",
      "ğŸ“¡ ì²­í¬ 3 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 9246ì)\n",
      "ğŸ“¡ ì²­í¬ 4 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 9369ì)\n",
      "ğŸ“¡ ì²­í¬ 5 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 9547ì)\n",
      "ğŸ“¡ ì²­í¬ 6 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 9461ì)\n",
      "ğŸ“¡ ì²­í¬ 7 ì„ë² ë”© ì¤‘... (ê¸¸ì´: 3636ì)\n",
      "âœ… ì™„ë£Œ: 7ê°œì˜ ê· ë“± ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "SOURCE_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ìš”ê¸ˆì œRAG\\KTìš”ê¸ˆì œì •ë³´_251209.docx\"\n",
    "OUTPUT_JSON_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\KT_plans_final_balanced.json\"\n",
    "\n",
    "print(\"ğŸ§  ëª¨ë¸ ë¡œë“œ ë° ê· ë“± ë¹„ìœ¨ ì²­í‚¹ ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def generate_balanced_7_chunks():\n",
    "    if not os.path.exists(SOURCE_PATH):\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = Document(SOURCE_PATH)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # [Step 1] ìš”ê¸ˆì œ ìœ ë‹› ì¶”ì¶œ ë° MD ë³€í™˜\n",
    "    md_units = []\n",
    "    current_unit = []\n",
    "    for para in paras:\n",
    "        if \"ìš”ê¸ˆì œë€?\" in para or para.startswith(\"[KT]\"):\n",
    "            if current_unit:\n",
    "                md_units.append(format_to_md(current_unit))\n",
    "                current_unit = []\n",
    "        current_unit.append(para)\n",
    "    if current_unit:\n",
    "        md_units.append(format_to_md(current_unit))\n",
    "\n",
    "    # [Step 2] ì „ì²´ ê¸€ì ìˆ˜ ê³„ì‚° ë° ëª©í‘œ ì²­í¬ í¬ê¸° ì„¤ì •\n",
    "    total_length = sum(len(u) for u in md_units)\n",
    "    target_chunk_size = total_length // 7\n",
    "    print(f\"ğŸ“Š ì „ì²´ ê¸€ì ìˆ˜: {total_length}ì / ëª©í‘œ ì²­í¬ í¬ê¸°: ì•½ {target_chunk_size}ì\")\n",
    "\n",
    "    # [Step 3] ê¸€ì ìˆ˜ ê¸°ë°˜ ê· ë“± ë°°ë¶„\n",
    "    final_chunks_text = []\n",
    "    current_chunk_content = []\n",
    "    current_chunk_length = 0\n",
    "\n",
    "    for unit in md_units:\n",
    "        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ ëª©í‘œ í¬ê¸°ì— ê°€ì¥ ê°€ê¹Œì›Œì§€ë„ë¡ ë°°ë¶„\n",
    "        # (ì´ë¯¸ ë°ì´í„°ê°€ ìˆê³ , ì¶”ê°€ ì‹œ ëª©í‘œë¥¼ í¬ê²Œ ìƒíšŒí•˜ë©´ ë‹¤ìŒ ì²­í¬ë¡œ ë„˜ê¹€)\n",
    "        if current_chunk_length > 0 and current_chunk_length + len(unit) > target_chunk_size + 1000:\n",
    "            if len(final_chunks_text) < 6: # 7ê°œê¹Œì§€ë§Œ ìƒì„±í•˜ë„ë¡ ì œì–´\n",
    "                final_chunks_text.append(\"\\n\\n---\\n\\n\".join(current_chunk_content))\n",
    "                current_chunk_content = [unit]\n",
    "                current_chunk_length = len(unit)\n",
    "                continue\n",
    "        \n",
    "        current_chunk_content.append(unit)\n",
    "        current_chunk_length += len(unit)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë‚¨ì€ ë‚´ìš© ì²˜ë¦¬\n",
    "    if current_chunk_content:\n",
    "        final_chunks_text.append(\"\\n\\n---\\n\\n\".join(current_chunk_content))\n",
    "\n",
    "    # [Step 4] JSON êµ¬ì„± ë° ì„ë² ë”©\n",
    "    json_data = []\n",
    "    for i, chunk_text in enumerate(final_chunks_text):\n",
    "        print(f\"ğŸ“¡ ì²­í¬ {i+1} ì„ë² ë”© ì¤‘... (ê¸¸ì´: {len(chunk_text)}ì)\")\n",
    "        dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "        \n",
    "        json_data.append({\n",
    "            \"pk\": f\"KT_CHUNK_{i+1}\",\n",
    "            \"source\": \"KT\",\n",
    "            \"text\": chunk_text,\n",
    "            \"dense\": dense_vec.tolist(),\n",
    "            \"file_hash\": \"KT_260107_BALANCED_7CHUNKS\",\n",
    "            \"page\": i + 1,\n",
    "            \"row\": 1\n",
    "        })\n",
    "\n",
    "    # [Step 5] ì €ì¥\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… ì™„ë£Œ: {len(json_data)}ê°œì˜ ê· ë“± ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "def format_to_md(unit):\n",
    "    \"\"\"ìœ ë‹› ë¦¬ìŠ¤íŠ¸ë¥¼ MD í˜•ì‹ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    md_lines = [f\"# {unit[0]}\"]\n",
    "    for line in unit[1:]:\n",
    "        md_lines.append(f\"## {line}\" if \":\" in line else f\"### {line}\")\n",
    "    return \"\\n\".join(md_lines)\n",
    "\n",
    "generate_balanced_7_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae92b68",
   "metadata": {},
   "source": [
    "# í†µì‹ ì‚¬_test_v3 íŒŒì¼ë¡œ MD ì²­í‚¹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87369a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì‹¬í”Œ MD ë³€í™˜ ë° ì„ë² ë”© ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\.cache\\huggingface\\modules\\transformers_modules\\nomic_hyphen_ai\\nomic_hyphen_bert_hyphen_2048\\7710840340a098cfb869c4f65e87cf2b1b70caca\\modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì´ 29ê°œì˜ ì‹¬í”Œ ì²­í¬ ìƒì„± ì™„ë£Œ. ì„ë² ë”© ì¤‘...\n",
      "âœ… ì™„ë£Œ: C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\jsoní´ë”\\SKT_simple_optimized.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from docx import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. ê²½ë¡œ ë° ì„¤ì •\n",
    "SOURCE_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ì‹œë‚˜ë¦¬ì˜¤2ë²ˆìš©RAG\\SKT_test_v3.docx\"\n",
    "OUTPUT_JSON_PATH = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\jsoní´ë”\\SKT_simple_optimized.json\"\n",
    "\n",
    "# ìµœì  ì²­í¬ ì°¾ê¸°\n",
    "MAX_CHARS_PER_CHUNK = 1000\n",
    "\n",
    "print(\"ğŸ§  ì‹¬í”Œ MD ë³€í™˜ ë° ì„ë² ë”© ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def format_simple_to_md(unit_paras):\n",
    "    \"\"\"[ìš”ê¸ˆì œ ì‹œì‘] ê¸°ë°˜ ë°ì´í„°ë¥¼ ## í˜•ì‹ì˜ MDë¡œ ë³€í™˜\"\"\"\n",
    "    if not unit_paras: return \"\"\n",
    "    \n",
    "    md_lines = []\n",
    "    for line in unit_paras:\n",
    "        # '[ìš”ê¸ˆì œ ì‹œì‘]' ë¬¸êµ¬ëŠ” ì œì™¸í•˜ê³  ì‹¤ì œ ì •ë³´ë§Œ ë‹´ìŠµë‹ˆë‹¤.\n",
    "        if \"[ìš”ê¸ˆì œ ì‹œì‘]\" in line:\n",
    "            continue\n",
    "        \n",
    "        # ëª¨ë“  í•­ëª© ì•ì— ##ë¥¼ ë¶™ì—¬ì„œ ì¼ê´€ëœ ê³„ì¸µ êµ¬ì¡° í˜•ì„±\n",
    "        if \":\" in line:\n",
    "            md_lines.append(f\"## {line}\")\n",
    "        else:\n",
    "            # í˜¹ì‹œ ì½œë¡ ì´ ì—†ëŠ” ì¤„ì´ ìˆë‹¤ë©´ ìƒì„¸ ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ### ì²˜ë¦¬\n",
    "            md_lines.append(f\"### {line}\")\n",
    "            \n",
    "    return \"\\n\".join(md_lines)\n",
    "\n",
    "def generate_simple_rag_json():\n",
    "    if not os.path.exists(SOURCE_PATH):\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SOURCE_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = Document(SOURCE_PATH)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    \n",
    "    # [Step 1] '[ìš”ê¸ˆì œ ì‹œì‘]' í‚¤ì›Œë“œë¡œ ìš”ê¸ˆì œë³„ ìœ ë‹› ë¶„ë¦¬\n",
    "    units = []\n",
    "    current_unit = []\n",
    "    for para in paras:\n",
    "        if \"[ìš”ê¸ˆì œ ì‹œì‘]\" in para:\n",
    "            if current_unit:\n",
    "                units.append(format_simple_to_md(current_unit))\n",
    "                current_unit = []\n",
    "        current_unit.append(para)\n",
    "    if current_unit:\n",
    "        units.append(format_simple_to_md(current_unit))\n",
    "\n",
    "    # [Step 2] ì„¤ì •í•œ ê¸€ì ìˆ˜(2000ì)ì— ë§ì¶° ë³‘í•©\n",
    "    final_chunks_text = []\n",
    "    temp_content = []\n",
    "    temp_length = 0\n",
    "\n",
    "    for md_text in units:\n",
    "        if temp_length + len(md_text) > MAX_CHARS_PER_CHUNK and temp_content:\n",
    "            final_chunks_text.append(\"\\n\\n---\\n\\n\".join(temp_content))\n",
    "            temp_content = [md_text]\n",
    "            temp_length = len(md_text)\n",
    "        else:\n",
    "            temp_content.append(md_text)\n",
    "            temp_length += (len(md_text) + 10) # êµ¬ë¶„ì ê°„ê²© ê³ ë ¤\n",
    "\n",
    "    if temp_content:\n",
    "        final_chunks_text.append(\"\\n\\n---\\n\\n\".join(temp_content))\n",
    "\n",
    "    # [Step 3] JSON êµ¬ì„± ë° ì„ë² ë”©\n",
    "    json_data = []\n",
    "    print(f\"ğŸ“Š ì´ {len(final_chunks_text)}ê°œì˜ ì‹¬í”Œ ì²­í¬ ìƒì„± ì™„ë£Œ. ì„ë² ë”© ì¤‘...\")\n",
    "    \n",
    "    for i, chunk_text in enumerate(final_chunks_text):\n",
    "        dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "        \n",
    "        json_data.append({\n",
    "            \"pk\": f\"SKT_SIM_{i+1}\",\n",
    "            \"source\": \"SKT\",\n",
    "            \"text\": chunk_text,\n",
    "            \"dense\": dense_vec.tolist(),\n",
    "            \"file_hash\": \"SKT_V3_SIMPLE_MD\",\n",
    "            \"page\": i + 1,\n",
    "            \"row\": 1\n",
    "        })\n",
    "\n",
    "    # [Step 4] ì €ì¥\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… ì™„ë£Œ: {OUTPUT_JSON_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_simple_rag_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
