{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a95a394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0eee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: 9ê°œ\n",
      "  - 112ì‹ ê³ .md\n",
      "  - ê°€ì‚¬ê·¼ë¡œì.md\n",
      "  - ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.md\n",
      "  - ê°„í˜¸ë²•.md\n",
      "  - ê°ì‚¬ì›ì§•ê³„.md\n",
      "  - ê°œì¸ì •ë³´í˜¸ë²•.md\n",
      "  - ë²•ì›ì„¤ì¹˜.md\n",
      "  - ì†Œë¹„ì„¸ë²•.md\n",
      "  - ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.md\n"
     ]
    }
   ],
   "source": [
    "# ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
    "MD_INPUT_DIR = r\"c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\test\\output\"\n",
    "JSON_OUTPUT_DIR = r\"c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\test\\jsoní´ë”\"\n",
    "JSON_OUTPUT_FILE = os.path.join(JSON_OUTPUT_DIR, \"ë²•ë¥ _ì„ë² ë”©.json\")\n",
    "\n",
    "# ì…ë ¥ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ ìˆ˜ì§‘\n",
    "md_files = sorted([f for f in os.listdir(MD_INPUT_DIR) if f.endswith('.md')])\n",
    "print(f\"ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(md_files)}ê°œ\")\n",
    "for fname in md_files:\n",
    "    print(f\"  - {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bb271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\test\\output\n",
      "ğŸ“ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìˆ˜: 9ê°œ\n",
      "ğŸ’¾ ì¶œë ¥ íŒŒì¼: c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\test\\jsoní´ë”\\ë²•ë¥ _ì„ë² ë”©.json\n",
      "âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "print(\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ì¶œë ¥ í´ë” ìƒì„±\n",
    "os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ì…ë ¥ ë””ë ‰í† ë¦¬: {MD_INPUT_DIR}\")\n",
    "print(f\"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìˆ˜: {len(md_files)}ê°œ\")\n",
    "print(f\"ì¶œë ¥ íŒŒì¼: {JSON_OUTPUT_FILE}\")\n",
    "print(f\"ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bae3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_from_ollama(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    SentenceTransformerë¥¼ í†µí•´ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: ì„ë² ë”© ë²¡í„°\n",
    "    \"\"\"\n",
    "    # Nomic ëª¨ë¸ ê¶Œì¥ ì ‘ë‘ì‚¬ ì¶”ê°€ (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)\n",
    "    input_text = f\"search_document: {text}\"\n",
    "    dense_vec = model.encode(input_text, normalize_embeddings=True)\n",
    "    \n",
    "    return dense_vec\n",
    "\n",
    "\n",
    "def parse_markdown_hierarchical(md_content: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ê³„ì¸µì  êµ¬ì¡°ë¡œ íŒŒì‹±\n",
    "    H1(ë²•ë¥ ëª…) > H2(ì¥) > H3(ì¡°) > H4(í•­) êµ¬ì¡° ìœ ì§€\n",
    "    H5 ë©”íƒ€ì •ë³´(ë‹´ë‹¹ë¶€ì„œ, ë²•ë ¹ì •ë³´, ì•½ì¹­, ë©”íƒ€ì •ë³´)ëŠ” í•„í„°ë§\n",
    "    \"\"\"\n",
    "    lines = md_content.split('\\n')\n",
    "    hierarchy = []\n",
    "    \n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    current_h5 = None\n",
    "    current_content = []\n",
    "    skip_next_lines = False  # H5 ë©”íƒ€ì •ë³´ ìŠ¤í‚µ í”Œë˜ê·¸\n",
    "    \n",
    "    for line in lines:\n",
    "        # í—¤ë” ë ˆë²¨ ê°ì§€\n",
    "        h1_match = re.match(r'^#\\s+(.+?)$', line)\n",
    "        h2_match = re.match(r'^##\\s+(.+?)$', line)\n",
    "        h3_match = re.match(r'^###\\s+(.+?)$', line)\n",
    "        h4_match = re.match(r'^####\\s+(.+?)$', line)\n",
    "        h5_match = re.match(r'^#####\\s+(.+?)$', line)\n",
    "        \n",
    "        # H5 ë©”íƒ€ì •ë³´ ê°ì§€ (ë‹´ë‹¹ë¶€ì„œ, ë²•ë ¹ì •ë³´, ì•½ì¹­, ë©”íƒ€ì •ë³´)\n",
    "        if h5_match:\n",
    "            h5_text = h5_match.group(1).strip()\n",
    "            if h5_text in ['ë‹´ë‹¹ë¶€ì„œ', 'ë²•ë ¹ì •ë³´', 'ì•½ì¹­', 'ë©”íƒ€ì •ë³´']:\n",
    "                skip_next_lines = True\n",
    "                continue\n",
    "        \n",
    "        # H5 ë©”íƒ€ì •ë³´ ì§í›„ ë³¸ë¬¸ì€ ìŠ¤í‚µ\n",
    "        if skip_next_lines and line.strip() and not re.match(r'^#{1,5}\\s+', line):\n",
    "            continue\n",
    "        elif skip_next_lines and (h1_match or h2_match or h3_match or h4_match):\n",
    "            skip_next_lines = False\n",
    "        \n",
    "        if h1_match:\n",
    "            current_h1 = h1_match.group(1).strip()\n",
    "            current_h2 = None\n",
    "            current_h3 = None\n",
    "            current_h4 = None\n",
    "            current_content = []\n",
    "            skip_next_lines = False\n",
    "        elif h2_match:\n",
    "            current_h2 = h2_match.group(1).strip()\n",
    "            current_h3 = None\n",
    "            current_h4 = None\n",
    "            current_content = []\n",
    "            skip_next_lines = False\n",
    "        elif h3_match:\n",
    "            # ì´ì „ H3ì˜ ë‚´ìš© ì €ì¥\n",
    "            if current_h3 and current_content:\n",
    "                hierarchy.append({\n",
    "                    'level': 3,\n",
    "                    'h1': current_h1,\n",
    "                    'h2': current_h2,\n",
    "                    'h3': current_h3,\n",
    "                    'h4': current_h4,\n",
    "                    'content': '\\n'.join(current_content).strip()\n",
    "                })\n",
    "            current_h3 = h3_match.group(1).strip()\n",
    "            current_h4 = None\n",
    "            current_content = []\n",
    "            skip_next_lines = False\n",
    "        elif h4_match:\n",
    "            # ì´ì „ H4ì˜ ë‚´ìš© ì €ì¥\n",
    "            if current_h4 and current_content:\n",
    "                hierarchy.append({\n",
    "                    'level': 4,\n",
    "                    'h1': current_h1,\n",
    "                    'h2': current_h2,\n",
    "                    'h3': current_h3,\n",
    "                    'h4': current_h4,\n",
    "                    'content': '\\n'.join(current_content).strip()\n",
    "                })\n",
    "            current_h4 = h4_match.group(1).strip()\n",
    "            current_content = []\n",
    "            skip_next_lines = False\n",
    "        elif line.strip() and not skip_next_lines:  # ë¹ˆ ì¤„ì´ ì•„ë‹ˆê³  H5 ìŠ¤í‚µ ì•„ë‹Œ ê²½ìš°\n",
    "            current_content.append(line)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ í•­ëª© ì €ì¥\n",
    "    if current_h4 and current_content:\n",
    "        hierarchy.append({\n",
    "            'level': 4,\n",
    "            'h1': current_h1,\n",
    "            'h2': current_h2,\n",
    "            'h3': current_h3,\n",
    "            'h4': current_h4,\n",
    "            'content': '\\n'.join(current_content).strip()\n",
    "        })\n",
    "    elif current_h3 and current_content:\n",
    "        hierarchy.append({\n",
    "            'level': 3,\n",
    "            'h1': current_h1,\n",
    "            'h2': current_h2,\n",
    "            'h3': current_h3,\n",
    "            'h4': None,\n",
    "            'content': '\\n'.join(current_content).strip()\n",
    "        })\n",
    "    \n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "def create_hierarchical_chunks(hierarchy_list: List[Dict], max_chars: int = 1500) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ê³„ì¸µì  êµ¬ì¡°ë¥¼ ì²­í¬ë¡œ ë³€í™˜\n",
    "    ê°™ì€ ì¡°(H3) ë‚´ì˜ ì—¬ëŸ¬ í•­(H4)ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë³‘í•© ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    temp_chunk = {\n",
    "        'h1': None,\n",
    "        'h2': None,\n",
    "        'h3': None,\n",
    "        'items': []  # H4 í•­ëª©ë“¤\n",
    "    }\n",
    "    temp_size = 0\n",
    "    \n",
    "    for item in hierarchy_list:\n",
    "        item_text = f\"{item.get('h4', '')}\\n{item['content']}\"\n",
    "        item_size = len(item_text)\n",
    "        \n",
    "        # ê°™ì€ ì¡°(H3)ì¸ ê²½ìš°\n",
    "        if temp_chunk['h3'] == item['h3'] and temp_size + item_size <= max_chars:\n",
    "            temp_chunk['items'].append(item)\n",
    "            temp_size += item_size\n",
    "        else:\n",
    "            # ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘\n",
    "            if temp_chunk['items']:\n",
    "                chunks.append(temp_chunk.copy())\n",
    "            temp_chunk = {\n",
    "                'h1': item['h1'],\n",
    "                'h2': item['h2'],\n",
    "                'h3': item['h3'],\n",
    "                'items': [item]\n",
    "            }\n",
    "            temp_size = item_size\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€\n",
    "    if temp_chunk['items']:\n",
    "        chunks.append(temp_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_to_text(chunk: Dict) -> str:\n",
    "    \"\"\"\n",
    "    ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (ì„ë² ë”©ìš©)\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if chunk['h1']:\n",
    "        text_parts.append(f\"# {chunk['h1']}\")\n",
    "    if chunk['h2']:\n",
    "        text_parts.append(f\"## {chunk['h2']}\")\n",
    "    if chunk['h3']:\n",
    "        text_parts.append(f\"### {chunk['h3']}\")\n",
    "    \n",
    "    for item in chunk['items']:\n",
    "        if item['h4']:\n",
    "            text_parts.append(f\"#### {item['h4']}\")\n",
    "        if item['content']:\n",
    "            text_parts.append(item['content'])\n",
    "    \n",
    "    return '\\n\\n'.join(text_parts)\n",
    "\n",
    "\n",
    "print(\"í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸° ë° íŒŒì‹± ì‹œì‘...\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: 112ì‹ ê³ .md\n",
      "  âœ“ 40ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°€ì‚¬ê·¼ë¡œì.md\n",
      "  âœ“ 16ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.md\n",
      "  âœ“ 50ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°„í˜¸ë²•.md\n",
      "  âœ“ 117ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°ì‚¬ì›ì§•ê³„.md\n",
      "  âœ“ 66ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°œì¸ì •ë³´í˜¸ë²•.md\n",
      "  âœ“ 372ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ë²•ì›ì„¤ì¹˜.md\n",
      "  âœ“ 4ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì†Œë¹„ì„¸ë²•.md\n",
      "  âœ“ 118ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.md\n",
      "  âœ“ 37ê°œ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "âœ… íŒŒì‹± ì™„ë£Œ: ì´ 820ê°œì˜ í•­ëª© ì¶”ì¶œ\n",
      "\n",
      "ğŸ“Š íŒŒì¼ë³„ í†µê³„:\n",
      "  - 112ì‹ ê³ .md: 40ê°œ í•­ëª©, 5.4 KB\n",
      "  - ê°€ì‚¬ê·¼ë¡œì.md: 16ê°œ í•­ëª©, 4.6 KB\n",
      "  - ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.md: 50ê°œ í•­ëª©, 16.1 KB\n",
      "  - ê°„í˜¸ë²•.md: 117ê°œ í•­ëª©, 20.3 KB\n",
      "  - ê°ì‚¬ì›ì§•ê³„.md: 66ê°œ í•­ëª©, 10.5 KB\n",
      "  - ê°œì¸ì •ë³´í˜¸ë²•.md: 372ê°œ í•­ëª©, 75.4 KB\n",
      "  - ë²•ì›ì„¤ì¹˜.md: 4ê°œ í•­ëª©, 1.3 KB\n",
      "  - ì†Œë¹„ì„¸ë²•.md: 118ê°œ í•­ëª©, 34.7 KB\n",
      "  - ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.md: 37ê°œ í•­ëª©, 6.7 KB\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸° ë° íŒŒì‹±\n",
    "print(\"ğŸ“– ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸° ë° íŒŒì‹± ì‹œì‘...\\n\")\n",
    "\n",
    "all_hierarchy = []\n",
    "file_stats = {}\n",
    "\n",
    "for md_file in md_files:\n",
    "    md_path = os.path.join(MD_INPUT_DIR, md_file)\n",
    "    print(f\"ğŸ“„ ì²˜ë¦¬ ì¤‘: {md_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            md_content = f.read()\n",
    "        \n",
    "        # ê³„ì¸µì  êµ¬ì¡°ë¡œ íŒŒì‹±\n",
    "        hierarchy = parse_markdown_hierarchical(md_content)\n",
    "        \n",
    "        # íŒŒì¼ë³„ í†µê³„\n",
    "        file_stats[md_file] = {\n",
    "            'items': len(hierarchy),\n",
    "            'size': len(md_content)\n",
    "        }\n",
    "        \n",
    "        # ì „ì—­ ê³„ì¸µ êµ¬ì¡°ì— ì¶”ê°€ (íŒŒì¼ëª… ì •ë³´ í¬í•¨)\n",
    "        for item in hierarchy:\n",
    "            item['source_file'] = md_file\n",
    "            all_hierarchy.append(item)\n",
    "        \n",
    "        print(f\"  âœ“ {len(hierarchy)}ê°œ í•­ëª© ì¶”ì¶œ\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— ì˜¤ë¥˜: {str(e)}\\n\")\n",
    "\n",
    "print(f\"íŒŒì‹± ì™„ë£Œ: ì´ {len(all_hierarchy)}ê°œì˜ í•­ëª© ì¶”ì¶œ\")\n",
    "print(f\"\\níŒŒì¼ë³„ í†µê³„:\")\n",
    "for fname, stats in file_stats.items():\n",
    "    print(f\"  - {fname}: {stats['items']}ê°œ í•­ëª©, {stats['size']/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ íŒŒì¼ ë‹¨ìœ„ ì²­í‚¹ ìˆ˜í–‰ ì¤‘...\n",
      "âœ… íŒŒì¼ ë‹¨ìœ„ ì²­í‚¹ ì™„ë£Œ: 9ê°œ ì²­í¬ ìƒì„±\n",
      "\n",
      "ğŸ“Š ì²­í¬ í¬ê¸° í†µê³„:\n",
      "   - ìµœì†Œ: 347 ê¸€ì\n",
      "   - ìµœëŒ€: 68592 ê¸€ì\n",
      "   - í‰ê· : 17281 ê¸€ì\n",
      "\n",
      "ğŸ“‹ íŒŒì¼ë³„ ì²­í¬ í¬ê¸°:\n",
      "   1. 112ì‹ ê³ .md: 4457 ê¸€ì\n",
      "   2. ê°€ì‚¬ê·¼ë¡œì.md: 3502 ê¸€ì\n",
      "   3. ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.md: 11963 ê¸€ì\n",
      "   4. ê°„í˜¸ë²•.md: 17524 ê¸€ì\n",
      "   5. ê°ì‚¬ì›ì§•ê³„.md: 9627 ê¸€ì\n",
      "   6. ê°œì¸ì •ë³´í˜¸ë²•.md: 68592 ê¸€ì\n",
      "   7. ë²•ì›ì„¤ì¹˜.md: 347 ê¸€ì\n",
      "   8. ì†Œë¹„ì„¸ë²•.md: 33046 ê¸€ì\n",
      "   9. ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.md: 6468 ê¸€ì\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ë‹¨ìœ„ ì²­í‚¹\n",
    "print(\"ğŸ“¦ íŒŒì¼ ë‹¨ìœ„ ì²­í‚¹ ìˆ˜í–‰ ì¤‘...\")\n",
    "\n",
    "chunks = []\n",
    "for md_file in md_files:\n",
    "    # í•´ë‹¹ íŒŒì¼ì˜ ëª¨ë“  ê³„ì¸µ í•­ëª©\n",
    "    file_items = [item for item in all_hierarchy if item['source_file'] == md_file]\n",
    "    \n",
    "    if not file_items:\n",
    "        continue\n",
    "    \n",
    "    # íŒŒì¼ ë‚´ ëª¨ë“  í•­ëª©ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë³‘í•©\n",
    "    chunk = {\n",
    "        'source_file': md_file,\n",
    "        'h1': file_items[0].get('h1'),  # ì²« H1 (ë²•ë¥ ëª…)\n",
    "        'h2': None,\n",
    "        'h3': None,\n",
    "        'items': file_items  # íŒŒì¼ì˜ ëª¨ë“  í•­ëª©\n",
    "    }\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(f\"âœ… íŒŒì¼ ë‹¨ìœ„ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "# ì²­í¬ í†µê³„\n",
    "chunk_sizes = [len(chunk_to_text(chunk)) for chunk in chunks]\n",
    "print(f\"\\n ì²­í¬ í¬ê¸° í†µê³„:\")\n",
    "print(f\"   - ìµœì†Œ: {min(chunk_sizes)} ê¸€ì\")\n",
    "print(f\"   - ìµœëŒ€: {max(chunk_sizes)} ê¸€ì\")\n",
    "print(f\"   - í‰ê· : {sum(chunk_sizes) / len(chunk_sizes):.0f} ê¸€ì\")\n",
    "print(f\"\\n íŒŒì¼ë³„ ì²­í¬ í¬ê¸°:\")\n",
    "for i, (fname, size) in enumerate(zip(md_files, chunk_sizes), 1):\n",
    "    print(f\"   {i}. {fname}: {size} ê¸€ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e1be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘...\n",
      "   [1/9] 112ì‹ ê³ .md: 40 í•­ëª©, 4457 ê¸€ì\n",
      "   [2/9] ê°€ì‚¬ê·¼ë¡œì.md: 16 í•­ëª©, 3502 ê¸€ì\n",
      "   [3/9] ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.md: 50 í•­ëª©, 11963 ê¸€ì\n",
      "   [4/9] ê°„í˜¸ë²•.md: 117 í•­ëª©, 17524 ê¸€ì\n",
      "   [5/9] ê°ì‚¬ì›ì§•ê³„.md: 66 í•­ëª©, 9627 ê¸€ì\n",
      "   [6/9] ê°œì¸ì •ë³´í˜¸ë²•.md: 372 í•­ëª©, 68592 ê¸€ì\n",
      "   [7/9] ë²•ì›ì„¤ì¹˜.md: 4 í•­ëª©, 347 ê¸€ì\n",
      "   [8/9] ì†Œë¹„ì„¸ë²•.md: 118 í•­ëª©, 33046 ê¸€ì\n",
      "   [9/9] ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.md: 37 í•­ëª©, 6468 ê¸€ì\n",
      "\n",
      "âœ… ì„ë² ë”© ì™„ë£Œ: 9ê°œì˜ ì²­í¬\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "print(\"ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "\n",
    "# íŒŒì¼ë³„ JSON ê²°ê³¼ ì €ì¥ì†Œ\n",
    "file_json_results = {}\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "    chunk_text = chunk_to_text(chunk)\n",
    "    \n",
    "    # ì„ë² ë”© ìƒì„±\n",
    "    dense_vec = get_embedding_from_ollama(chunk_text)\n",
    "    \n",
    "    source_file = chunk['source_file']\n",
    "    \n",
    "    # íŒŒì¼ í•´ì‹œ ìƒì„±\n",
    "    file_hash = hashlib.md5(source_file.encode()).hexdigest()\n",
    "    \n",
    "    # JSON í•­ëª© ìƒì„±\n",
    "    json_item = {\n",
    "        \"pk\": f\"LAW_{chunk_idx+1:03d}\",\n",
    "        \"source\": source_file,\n",
    "        \"text\": chunk_text,\n",
    "        \"dense\": dense_vec.tolist(),\n",
    "        \"file_hash\": file_hash,\n",
    "        \"page\": 1,\n",
    "        \"row\": chunk_idx + 1\n",
    "    }\n",
    "    \n",
    "    # íŒŒì¼ë³„ë¡œ ë¶„ë¥˜\n",
    "    if source_file not in file_json_results:\n",
    "        file_json_results[source_file] = []\n",
    "    \n",
    "    file_json_results[source_file].append(json_item)\n",
    "    \n",
    "    print(f\"   [{chunk_idx+1}/{len(chunks)}] {source_file}: {len(chunk['items'])} í•­ëª©, {len(chunk_text)} ê¸€ì\")\n",
    "\n",
    "print(f\"\\n ì„ë² ë”© ì™„ë£Œ: {len(chunks)}ê°œì˜ ì²­í¬\")\n",
    "\n",
    "# í¸ì˜ìƒ ì „ì²´ ê²°ê³¼ë„ ë³´ê´€\n",
    "json_results = [item for items in file_json_results.values() for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f6c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘: c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\test\\jsoní´ë”\\ë²•ë¥ _ì„ë² ë”©.json\n",
      "âœ… JSON ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 0.53 MB\n"
     ]
    }
   ],
   "source": [
    "# JSON ì €ì¥\n",
    "print(f\"JSON íŒŒì¼ ì €ì¥ ì¤‘: {JSON_OUTPUT_FILE}\")\n",
    "with open(JSON_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"JSON ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "file_size_mb = os.path.getsize(JSON_OUTPUT_FILE) / (1024 * 1024)\n",
    "print(f\"íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ ìƒì„±ëœ JSON êµ¬ì¡°:\n",
      "\n",
      "ì´ ì²­í¬ ìˆ˜: 9\n",
      "\n",
      "ì²« ë²ˆì§¸ ì²­í¬ ìƒ˜í”Œ:\n",
      "  PK: LAW_CHUNK_001\n",
      "  ì›ë³¸ íŒŒì¼: 112ì‹ ê³ .md\n",
      "  ë²•ë¥ ëª…: None\n",
      "  í¬í•¨ í•­ëª©: 40ê°œ\n",
      "  Text ê¸¸ì´: 4457 ê¸€ì\n",
      "  Embedding ì°¨ì›: 768\n",
      "\n",
      "ğŸ“Š ìµœì¢… í†µê³„:\n",
      "  - ì²˜ë¦¬ íŒŒì¼: 9ê°œ\n",
      "  - ì¶”ì¶œ í•­ëª©: 820ê°œ\n",
      "  - ìƒì„± ì²­í¬: 9ê°œ (íŒŒì¼ ë‹¨ìœ„)\n",
      "  - í‰ê·  ì²­í¬ í¬ê¸°: 17281 ê¸€ì\n",
      "  - ì„ë² ë”© ì°¨ì›: 768\n",
      "\n",
      "âœ… ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ê²€ì¦\n",
    "print(\"\\n ìƒì„±ëœ JSON êµ¬ì¡°:\")\n",
    "print(f\"\\nì´ ì²­í¬ ìˆ˜: {len(json_results)}\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ì²­í¬ ìƒ˜í”Œ:\")\n",
    "sample_chunk = json_results[0]\n",
    "print(f\"  PK: {sample_chunk['pk']}\")\n",
    "print(f\"  ì›ë³¸ íŒŒì¼: {sample_chunk['source_file']}\")\n",
    "print(f\"  ë²•ë¥ ëª…: {sample_chunk['h1']}\")\n",
    "print(f\"  í¬í•¨ í•­ëª©: {sample_chunk['item_count']}ê°œ\")\n",
    "print(f\"  Text ê¸¸ì´: {len(sample_chunk['text'])} ê¸€ì\")\n",
    "print(f\"  Embedding ì°¨ì›: {len(sample_chunk['dense'])}\")\n",
    "\n",
    "# í†µê³„\n",
    "print(f\"\\n ìµœì¢… í†µê³„:\")\n",
    "print(f\"  - ì²˜ë¦¬ íŒŒì¼: {len(md_files)}ê°œ\")\n",
    "print(f\"  - ì¶”ì¶œ í•­ëª©: {len(all_hierarchy)}ê°œ\")\n",
    "print(f\"  - ìƒì„± ì²­í¬: {len(json_results)}ê°œ (íŒŒì¼ ë‹¨ìœ„)\")\n",
    "print(f\"  - í‰ê·  ì²­í¬ í¬ê¸°: {sum(chunk_sizes) / len(chunk_sizes):.0f} ê¸€ì\")\n",
    "print(f\"  - ì„ë² ë”© ì°¨ì›: {len(sample_chunk['dense'])}\")\n",
    "\n",
    "print(\"\\n ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” ì¿¼ë¦¬: 'ê³ ìš©ë³´í—˜ ê°€ì…'\n",
      "============================================================\n",
      "\n",
      "ğŸ“Œ ìƒìœ„ 3ê°œ ê´€ë ¨ ì²­í¬:\n",
      "\n",
      "[1] ìœ ì‚¬ë„: 0.4918\n",
      "    íŒŒì¼: ê°€ì‚¬ê·¼ë¡œì.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: None\n",
      "    ì¡°: ì œ1ì¡°(ëª©ì ) ì´ ì˜ì€ ã€Œê°€ì‚¬ê·¼ë¡œìì˜ ê³ ìš©ê°œì„  ë“±ì— ê´€í•œ ë²•ë¥ ã€ì—ì„œ ìœ„ì„ëœ ì‚¬í•­ê³¼ ê·¸ ì‹œí–‰ì— í•„ìš”í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„\n",
      "    ë‚´ìš©: ### ì œ1ì¡°(ëª©ì ) ì´ ì˜ì€ ã€Œê°€ì‚¬ê·¼ë¡œìì˜ ê³ ìš©ê°œì„  ë“±ì— ê´€í•œ ë²•ë¥ ã€ì—ì„œ ìœ„ì„ëœ ì‚¬í•­ê³¼ ê·¸ ì‹œí–‰ì— í•„ìš”í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„\n",
      "\n",
      "#### ì œ1í•­\n",
      "\n",
      "ì œ2ì¡°(ê°€ì‚¬ì„œë¹„ìŠ¤ ì œê³µê¸°ê´€ì˜ ì¸ì¦ ìš”ê±´) ã€Œê°€ì‚¬ê·¼ë¡œìì˜ ê³ ìš©ê°œì„  ë“±ì— ê´€...\n",
      "\n",
      "[2] ìœ ì‚¬ë„: 0.4620\n",
      "    íŒŒì¼: ê°€ì‚¬ê·¼ë¡œì.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: None\n",
      "    ì¡°: ì œ9ì¡°(ê¶Œí•œì˜ ìœ„ì„) ê³ ìš©ë…¸ë™ë¶€ì¥ê´€ì€ ë²• ì œ24ì¡°ì œ1í•­ì— ë”°ë¼ ë‹¤ìŒ ê° í˜¸ì˜ ì‚¬í•­ì— ê´€í•œ ê¶Œí•œì„ ì§€ë°©ê³ ìš©ë…¸ë™ê´€ì„œì˜\n",
      "    ë‚´ìš©: ### ì œ9ì¡°(ê¶Œí•œì˜ ìœ„ì„) ê³ ìš©ë…¸ë™ë¶€ì¥ê´€ì€ ë²• ì œ24ì¡°ì œ1í•­ì— ë”°ë¼ ë‹¤ìŒ ê° í˜¸ì˜ ì‚¬í•­ì— ê´€í•œ ê¶Œí•œì„ ì§€ë°©ê³ ìš©ë…¸ë™ê´€ì„œì˜\n",
      "\n",
      "#### ì œ1í•­\n",
      "\n",
      "ì œ10ì¡°(ì—…ë¬´ì˜ ìœ„íƒ) ë²• ì œ24ì¡°ì œ2í•­ì œ3í˜¸ì—ì„œ â€œëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ì—…...\n",
      "\n",
      "[3] ìœ ì‚¬ë„: 0.4561\n",
      "    íŒŒì¼: ê°€ì‚¬ê·¼ë¡œì.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: None\n",
      "    ì¡°: ì œ4ì¡°(ê·¼ë¡œê³„ì•½ ì²´ê²° ì‹œ ëª…ì‹œí•´ì•¼ í•  ê·¼ë¡œì¡°ê±´) ë²• ì œ14ì¡°ì œ1í•­ì œ5í˜¸ì—ì„œ â€œëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ê·¼ë¡œì¡°ê±´ì— ê´€í•œ ì‚¬í•­\n",
      "    ë‚´ìš©: ### ì œ4ì¡°(ê·¼ë¡œê³„ì•½ ì²´ê²° ì‹œ ëª…ì‹œí•´ì•¼ í•  ê·¼ë¡œì¡°ê±´) ë²• ì œ14ì¡°ì œ1í•­ì œ5í˜¸ì—ì„œ â€œëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ê·¼ë¡œì¡°ê±´ì— ê´€í•œ ì‚¬í•­\n",
      "\n",
      "â€ì´ë€ ë‹¤ìŒ ê° í˜¸ì˜ ì‚¬í•­ì„ ë§í•œë‹¤.\n",
      "ê°€ì‚¬ê·¼ë¡œìì˜ ê³ ìš©ê°œì„  ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹\n",
      "- 1...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ” ì¿¼ë¦¬: 'ì‹œí–‰ì¼'\n",
      "============================================================\n",
      "\n",
      "ğŸ“Œ ìƒìœ„ 3ê°œ ê´€ë ¨ ì²­í¬:\n",
      "\n",
      "[1] ìœ ì‚¬ë„: 0.5472\n",
      "    íŒŒì¼: ê°„í˜¸ë²•.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: ë¶€ì¹™\n",
      "    ì¡°: ì œ1ì¡°(ì‹œí–‰ì¼) ì´ ë²•ì€ ê³µí¬ í›„ 9ê°œì›”ì´ ê²½ê³¼í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤. ë‹¤ë§Œ, ì œ14ì¡°ì œ2í•­ì— ë”°ë¥¸ êµìœ¡ê³¼ì • ìš´ì˜ê¸°ê´€ì˜ ì§€\n",
      "    ë‚´ìš©: ## ë¶€ì¹™\n",
      "\n",
      "### ì œ1ì¡°(ì‹œí–‰ì¼) ì´ ë²•ì€ ê³µí¬ í›„ 9ê°œì›”ì´ ê²½ê³¼í•œ ë‚ ë¶€í„° ì‹œí–‰í•œë‹¤. ë‹¤ë§Œ, ì œ14ì¡°ì œ2í•­ì— ë”°ë¥¸ êµìœ¡ê³¼ì • ìš´ì˜ê¸°ê´€ì˜ ì§€\n",
      "\n",
      "ì • ë° í‰ê°€ëŠ” ê³µí¬ í›„ 9ê°œì›”ì´ ê²½ê³¼í•œ ë‚ ë¶€í„° 3ë…„ ë‚´ì— ì‹œí–‰í•˜ë˜, ê·¸ ê¸°...\n",
      "\n",
      "[2] ìœ ì‚¬ë„: 0.5383\n",
      "    íŒŒì¼: ê°„í˜¸ë²•.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: ë¶€ì¹™\n",
      "    ì¡°: ì œ10ì¡°(ë‹¤ë¥¸ ë²•ë ¹ê³¼ì˜ ê´€ê³„) ì´ ë²• ì‹œí–‰ ë‹¹ì‹œ ë‹¤ë¥¸ ë²•ë ¹ì—ì„œ ì¢…ì „ì˜ ã€Œì˜ë£Œë²•ã€ ë˜ëŠ” ê·¸ ê·œì •ì„ ì¸ìš©í•œ ê²½ìš° ì´ ë²• ê°€ìš´\n",
      "    ë‚´ìš©: ## ë¶€ì¹™\n",
      "\n",
      "### ì œ10ì¡°(ë‹¤ë¥¸ ë²•ë ¹ê³¼ì˜ ê´€ê³„) ì´ ë²• ì‹œí–‰ ë‹¹ì‹œ ë‹¤ë¥¸ ë²•ë ¹ì—ì„œ ì¢…ì „ì˜ ã€Œì˜ë£Œë²•ã€ ë˜ëŠ” ê·¸ ê·œì •ì„ ì¸ìš©í•œ ê²½ìš° ì´ ë²• ê°€ìš´\n",
      "\n",
      "ë° ê·¸ì— í•´ë‹¹í•˜ëŠ” ê·œì •ì´ ìˆìœ¼ë©´ ì¢…ì „ì˜ ã€Œì˜ë£Œë²•ã€ ë˜ëŠ” ê·¸ ê·œì •ì„ ê°ˆìŒí•˜...\n",
      "\n",
      "[3] ìœ ì‚¬ë„: 0.5307\n",
      "    íŒŒì¼: ê°„í˜¸ë²•.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: ë¶€ì¹™\n",
      "    ì¡°: ì œ7ì¡°(ì²˜ë¶„ ë“±ì— ê´€í•œ ê²½ê³¼ì¡°ì¹˜) ì´ ë²• ì‹œí–‰ ë‹¹ì‹œ ì¢…ì „ì˜ í–‰ì •ê¸°ê´€ì˜ í–‰ìœ„ë‚˜ í–‰ì •ê¸°ê´€ì— ëŒ€í•œ í–‰ìœ„ëŠ” ì´ ë²•ì— ê·¸ì— í•´\n",
      "    ë‚´ìš©: ## ë¶€ì¹™\n",
      "\n",
      "### ì œ7ì¡°(ì²˜ë¶„ ë“±ì— ê´€í•œ ê²½ê³¼ì¡°ì¹˜) ì´ ë²• ì‹œí–‰ ë‹¹ì‹œ ì¢…ì „ì˜ í–‰ì •ê¸°ê´€ì˜ í–‰ìœ„ë‚˜ í–‰ì •ê¸°ê´€ì— ëŒ€í•œ í–‰ìœ„ëŠ” ì´ ë²•ì— ê·¸ì— í•´\n",
      "\n",
      "ë‹¹í•˜ëŠ” ê·œì •ì´ ìˆìœ¼ë©´ ì´ ë²•ì— ë”°ë¥¸ í–‰ì •ê¸°ê´€ì˜ í–‰ìœ„ë‚˜ í–‰ì •ê¸°ê´€ì— ëŒ€í•œ í–‰ìœ„ë¡œ...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ” ì¿¼ë¦¬: 'ë‹´ë‹¹ë¶€ì„œ ì—°ë½ì²˜'\n",
      "============================================================\n",
      "\n",
      "ğŸ“Œ ìƒìœ„ 3ê°œ ê´€ë ¨ ì²­í¬:\n",
      "\n",
      "[1] ìœ ì‚¬ë„: 0.4689\n",
      "    íŒŒì¼: ê°œì¸ì •ë³´í˜¸ë²•.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: ì œ3ì¥ ê°œì¸ì •ë³´ì˜ ì²˜ë¦¬\n",
      "    ì¡°: None\n",
      "    ë‚´ìš©: ## ì œ3ì¥ ê°œì¸ì •ë³´ì˜ ì²˜ë¦¬\n",
      "\n",
      "#### ì œ1í•­\n",
      "\n",
      "ì œ17ì¡°(ê°œì¸ì •ë³´ì˜ ì œê³µ) ê°œì¸ì •ë³´ì²˜ë¦¬ìëŠ” ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹ë˜ëŠ” ê²½ìš°ì—ëŠ” ì •ë³´ì£¼ì²´ì˜ ê°œì¸ì •ë³´ë¥¼\n",
      "ì œ3ìì—ê²Œ ì œê³µ(ê³µìœ ë¥¼ í¬í•¨í•œë‹¤. ì´í•˜ ê°™ë‹¤)í•  ìˆ˜ ìˆë‹¤....\n",
      "\n",
      "[2] ìœ ì‚¬ë„: 0.4655\n",
      "    íŒŒì¼: ê°œì¸ì •ë³´í˜¸ë²•.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: ì œ3ì¥ ê°œì¸ì •ë³´ì˜ ì²˜ë¦¬\n",
      "    ì¡°: None\n",
      "    ë‚´ìš©: ## ì œ3ì¥ ê°œì¸ì •ë³´ì˜ ì²˜ë¦¬\n",
      "\n",
      "#### ì œ3í•­\n",
      "\n",
      "ê°œì¸ì •ë³´ì²˜ë¦¬ìëŠ” ì œ2í•­ì œ1í˜¸ì— ë”°ë¥¸ ë™ì˜ë¥¼ ë°›ì„ ë•Œì—ëŠ” ë‹¤ìŒ ê° í˜¸ì˜ ì‚¬í•­ì„ ì •ë³´ì£¼ì²´ì—ê²Œ ì•Œë ¤ì•¼ í•œë‹¤. ë‹¤ìŒ\n",
      "ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì˜ ì‚¬í•­ì„ ë³€ê²½í•˜ëŠ” ê²½ìš°ì—ë„ ì´ë¥¼ ì•Œë¦¬ê³ ...\n",
      "\n",
      "[3] ìœ ì‚¬ë„: 0.4529\n",
      "    íŒŒì¼: ê°œì¸ì •ë³´í˜¸ë²•.md\n",
      "    ë²•ë¥ : None\n",
      "    ì¥: ì œ3ì¥ ê°œì¸ì •ë³´ì˜ ì²˜ë¦¬\n",
      "    ì¡°: ì œ28ì¡°ì˜7(ì ìš©ë²”ìœ„) ì œ28ì¡°ì˜2 ë˜ëŠ” ì œ28ì¡°ì˜3ì— ë”°ë¼ ì²˜ë¦¬ëœ ê°€ëª…ì •ë³´ëŠ” ì œ20ì¡°, ì œ20ì¡°ì˜2, ì œ27ì¡°, ì œ34ì¡°ì œ1í•­,\n",
      "    ë‚´ìš©: ## ì œ3ì¥ ê°œì¸ì •ë³´ì˜ ì²˜ë¦¬\n",
      "\n",
      "### ì œ28ì¡°ì˜7(ì ìš©ë²”ìœ„) ì œ28ì¡°ì˜2 ë˜ëŠ” ì œ28ì¡°ì˜3ì— ë”°ë¼ ì²˜ë¦¬ëœ ê°€ëª…ì •ë³´ëŠ” ì œ20ì¡°, ì œ20ì¡°ì˜2, ì œ27ì¡°, ì œ34ì¡°ì œ1í•­,\n",
      "\n",
      "#### ì œ1í•­\n",
      "\n",
      "ì œ28ì¡°ì˜8(ê°œì¸ì •ë³´ì˜ êµ­ì™¸...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì¿¼ë¦¬ ì˜ˆì‹œ\n",
    "queries = [\n",
    "    \"ê³ ìš©ë³´í—˜ ê°€ì…\",\n",
    "    \"ì‹œí–‰ì¼\",\n",
    "    \"ë‹´ë‹¹ë¶€ì„œ ì—°ë½ì²˜\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" ì¿¼ë¦¬: '{query}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    query_embedding = model.encode([f\"search_document: {query}\"], normalize_embeddings=True)[0]\n",
    "    \n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = []\n",
    "    for chunk in json_results:\n",
    "        chunk_embedding = np.array(chunk['dense'])\n",
    "        similarity = np.dot(query_embedding, chunk_embedding)\n",
    "        similarities.append((similarity, chunk))\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ê²°ê³¼\n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    print(\"\\n ìƒìœ„ 3ê°œ ê´€ë ¨ ì²­í¬:\\n\")\n",
    "    for i, (score, chunk) in enumerate(similarities[:3], 1):\n",
    "        print(f\"[{i}] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "        print(f\"    íŒŒì¼: {chunk['source_file']}\")\n",
    "        print(f\"    ë²•ë¥ : {chunk['h1']}\")\n",
    "        print(f\"    ì¥: {chunk['h2']}\")\n",
    "        print(f\"    ì¡°: {chunk['h3']}\")\n",
    "        print(f\"    ë‚´ìš©: {chunk['text'][:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0196cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: 112ì‹ ê³ .md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: 112ì‹ ê³ .json (8ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°€ì‚¬ê·¼ë¡œì.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ê°€ì‚¬ê·¼ë¡œì.json (7ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ê°„ì„ ê¸‰ë²„ìŠ¤ìš´í–‰.json (25ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°„í˜¸ë²•.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ê°„í˜¸ë²•.json (29ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°ì‚¬ì›ì§•ê³„.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ê°ì‚¬ì›ì§•ê³„.json (15ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ê°œì¸ì •ë³´í˜¸ë²•.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ê°œì¸ì •ë³´í˜¸ë²•.json (112ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ë²•ì›ì„¤ì¹˜.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ë²•ì›ì„¤ì¹˜.json (2ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì†Œë¹„ì„¸ë²•.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ì†Œë¹„ì„¸ë²•.json (51ê°œ ì²­í¬)\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.md\n",
      "  âœ… ì €ì¥ ì™„ë£Œ: ì˜¨ì‹¤ê°€ìŠ¤ë°°ì¶œ.json (11ê°œ ì²­í¬)\n",
      "\n",
      "ğŸš€ ëª¨ë“  íŒŒì¼ì˜ ê°œë³„ JSON ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "MD_INPUT_DIR = r\"./output\" \n",
    "JSON_OUTPUT_DIR = r\"./json_result\"\n",
    "os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 2. ëª¨ë¸ë¡œë“œ (ì‚¬ìš©ì ëª¨ë¸ ìœ ì§€)\n",
    "print(\" ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì„¤ì • (1000ì ì œí•œ + ë¬¸ë§¥ ë³´ì¡´)\n",
    "# ë¬¸ë‹¨( \\n\\n ), ì¤„ë°”ê¿ˆ( \\n ), ë¬¸ì¥( . ), ê³µë°± ìˆœìœ¼ë¡œ ê²½ê³„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,  # ë¬¸ë§¥ ì—°ê²°ì„ ìœ„í•´ ì•ë’¤ 100ì ì •ë„ ê²¹ì¹¨ í—ˆìš©\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "def process_md_to_json():\n",
    "    md_files = [f for f in os.listdir(MD_INPUT_DIR) if f.endswith('.md')]\n",
    "    \n",
    "    for md_file in md_files:\n",
    "        print(f\"ğŸ“„ ì²˜ë¦¬ ì¤‘: {md_file}\")\n",
    "        md_path = os.path.join(MD_INPUT_DIR, md_file)\n",
    "        \n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # íŒŒì¼ ì „ì²´ì— ëŒ€í•œ í•´ì‹œ ìƒì„± (íŒŒì¼ ì‹ë³„ìš©)\n",
    "        file_hash = hashlib.md5(content.encode()).hexdigest()\n",
    "        \n",
    "        # 1000ì ë‹¨ìœ„ë¡œ ì§€ëŠ¥í˜• ë¶„í• \n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        json_results = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            # ì„ë² ë”© ìƒì„±\n",
    "            dense_vec = model.encode(f\"search_document: {chunk_text}\", normalize_embeddings=True)\n",
    "            \n",
    "            # PK ìƒì„± (íŒŒì¼ëª…_ìˆœë²ˆ) - ë™ì¼ íŒŒì¼ ë‚´ ìˆœì„œ ì‹ë³„\n",
    "            file_name_only = os.path.splitext(md_file)[0]\n",
    "            pk = f\"{file_name_only}_{i+1:03d}\"\n",
    "            \n",
    "            # ë°ì´í„° êµ¬ì¡°í™”\n",
    "            item = {\n",
    "                \"pk\": pk,\n",
    "                \"source\": md_file,          # ì›ë³¸ íŒŒì¼ ì‹ë³„\n",
    "                \"text\": chunk_text,\n",
    "                \"dense\": dense_vec.tolist(),\n",
    "                \"file_hash\": file_hash,     # íŒŒì¼ ë²„ì „/ë‚´ìš© ì‹ë³„\n",
    "                \"page\": 1,                  # MD íŠ¹ì„±ìƒ 1ë¡œ ê³ ì •\n",
    "                \"row\": i + 1                # íŒŒì¼ ë‚´ ìˆœì„œ\n",
    "            }\n",
    "            json_results.append(item)\n",
    "        \n",
    "        # íŒŒì¼ë³„ ê°œë³„ JSON ì €ì¥\n",
    "        output_filename = f\"{file_name_only}.json\"\n",
    "        output_path = os.path.join(JSON_OUTPUT_DIR, output_filename)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_results, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"  ì €ì¥ ì™„ë£Œ: {output_filename} ({len(chunks)}ê°œ ì²­í¬)\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    process_md_to_json()\n",
    "    print(\"\\n ëª¨ë“  íŒŒì¼ì˜ ê°œë³„ JSON ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
