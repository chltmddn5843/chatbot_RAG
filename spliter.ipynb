{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57eba6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import re\n",
    "\n",
    "def reformat_telecom_plans(docx_path, output_path):\n",
    "    try:\n",
    "        # 1. docx íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        doc = Document(docx_path)\n",
    "        full_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                full_text.append(para.text)\n",
    "        \n",
    "        content = \"\\n\".join(full_text)\n",
    "\n",
    "        # 2. íƒœê·¸ ì œê±°\n",
    "        clean_text = re.sub(r'\\\\s*', '', content)\n",
    "        \n",
    "        # 3. ìš”ê¸ˆì œëª… í˜•ì‹ ìˆ˜ì •\n",
    "        lines = [line.strip() for line in clean_text.split('\\n') if line.strip()]\n",
    "        formatted_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith(\"ìš”ê¸ˆì œëª…\"):\n",
    "                if formatted_lines:\n",
    "                    formatted_lines.append(\"\") # ìš”ê¸ˆì œ ì‚¬ì´ ê°„ê²©\n",
    "                formatted_lines.append(f\"# {line}\")\n",
    "            else:\n",
    "                # í•­ëª©ë³„ ë¶ˆë¦¿ í¬ì¸íŠ¸ ì¶”ê°€ (ì„ íƒ ì‚¬í•­, ì›ì¹˜ ì•Šìœ¼ì‹œë©´ ì œê±° ê°€ëŠ¥)\n",
    "                formatted_lines.append(f\"* {line}\")\n",
    "        \n",
    "        # 4. ê²°ê³¼ ì €ì¥\n",
    "        result = \"\\n\".join(formatted_lines)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result)\n",
    "            \n",
    "        print(f\"ë³€í™˜ ì„±ê³µ! íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "# --- ì‹¤í–‰ ë¶€ë¶„ ---\n",
    "# ì…ë ¥í•˜ì‹  ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ë‹´ì•„ ì‹¤í–‰í•©ë‹ˆë‹¤. (r ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ ê²½ë¡œ ì˜¤ë¥˜ ë°©ì§€)\n",
    "input_path = r'C:/Users/ë¯¸ì†Œì •ë³´ê¸°ìˆ /OneDrive - ì¸í•˜ëŒ€í•™êµ/ë°”íƒ• í™”ë©´/RAGíŒŒì¼/ì‹œë‚˜ë¦¬ì˜¤2ë²ˆìš©RAG/ê¸°íƒ€í…ŒìŠ¤íŠ¸ë²„ì „/lgu+_test_v2.docx'\n",
    "output_path = r'C:/Users/ë¯¸ì†Œì •ë³´ê¸°ìˆ /OneDrive - ì¸í•˜ëŒ€í•™êµ/ë°”íƒ• í™”ë©´/RAGíŒŒì¼/ì‹œë‚˜ë¦¬ì˜¤2ë²ˆìš©RAG/ê¸°íƒ€í…ŒìŠ¤íŠ¸ë²„ì „/lgu+.md'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "212e6307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³€í™˜ ì„±ê³µ! íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: C:/Users/ë¯¸ì†Œì •ë³´ê¸°ìˆ /OneDrive - ì¸í•˜ëŒ€í•™êµ/ë°”íƒ• í™”ë©´/RAGíŒŒì¼/ì‹œë‚˜ë¦¬ì˜¤2ë²ˆìš©RAG/ê¸°íƒ€í…ŒìŠ¤íŠ¸ë²„ì „/lgu+.md\n"
     ]
    }
   ],
   "source": [
    "reformat_telecom_plans(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e995f",
   "metadata": {},
   "source": [
    "## MDë¬¸ì„œë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e35994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ì¹´ë“œì‚¬ ì •ë³´ê°€ í¬í•¨ëœ MD ë¬¸ì„œ ìƒì„± ì‹œì‘...\n",
      "âœ… [ì‚¼ì„±ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [KBêµ­ë¯¼ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ë¡¯ë°ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ì‹ í•œì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [NHë†í˜‘ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [í•˜ë‚˜ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [IBKê¸°ì—…ì€í–‰] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ìš°ë¦¬ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [BCë°”ë¡œì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [í˜„ëŒ€ì¹´ë“œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [iMë±…í¬] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ì¼€ì´ë±…í¬] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [BNKë¶€ì‚°ì€í–‰] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ê´‘ì£¼ì€í–‰] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ì œì£¼ì€í–‰] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [í† ìŠ¤ë±…í¬] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ìš°ì²´êµ­] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n",
      "âœ… [ë¯¸ë˜ì—ì…‹ì¦ê¶Œ] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# --- ê²½ë¡œ ì„¤ì • ---\n",
    "SOURCE_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ì‹œë‚˜ë¦¬ì˜¤3ë²ˆìš©RAG\\ì¹´ë“œ_ì€í–‰_0102\"\n",
    "MD_OUTPUT_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ì‹œë‚˜ë¦¬ì˜¤3ë²ˆìš©RAG\\MD_ê°œë³„íŒŒì¼\"\n",
    "\n",
    "# ì²˜ë¦¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸\n",
    "card_list = [\n",
    "    \"ì‚¼ì„±ì¹´ë“œ\", \"KBêµ­ë¯¼ì¹´ë“œ\", \"ë¡¯ë°ì¹´ë“œ\", \"ì‹ í•œì¹´ë“œ\", \"NHë†í˜‘ì¹´ë“œ\", \"í•˜ë‚˜ì¹´ë“œ\",\n",
    "    \"IBKê¸°ì—…ì€í–‰\", \"ìš°ë¦¬ì¹´ë“œ\", \"BCë°”ë¡œì¹´ë“œ\", \"í˜„ëŒ€ì¹´ë“œ\", \"iMë±…í¬\", \"ì¼€ì´ë±…í¬\",\n",
    "    \"BNKë¶€ì‚°ì€í–‰\", \"ê´‘ì£¼ì€í–‰\", \"ì œì£¼ì€í–‰\", \"í† ìŠ¤ë±…í¬\", \"ìš°ì²´êµ­\", \"ë¯¸ë˜ì—ì…‹ì¦ê¶Œ\"\n",
    "]\n",
    "\n",
    "def finalize_unit(unit_dict, card_company):\n",
    "    \"\"\"\n",
    "    ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •í˜•í™”ëœ MD í…ìŠ¤íŠ¸ í¬ë§·ìœ¼ë¡œ ë³€í™˜\n",
    "    'ì¹´ë“œì‚¬' í•­ëª©ì„ ì¶”ê°€í•˜ì—¬ ì¶œì²˜ë¥¼ ëª…í™•íˆ í•¨\n",
    "    \"\"\"\n",
    "    ott_combined = \", \".join(unit_dict[\"OTT_í˜œíƒ\"]) if unit_dict[\"OTT_í˜œíƒ\"] else \"ì •ë³´ ì—†ìŒ\"\n",
    "    annual_fee = unit_dict[\"ì—°íšŒë¹„\"] if unit_dict[\"ì—°íšŒë¹„\"] else \"ì •ë³´ ì—†ìŒ\"\n",
    "    target = unit_dict[\"ì¶”ì²œ_ëŒ€ìƒ\"] if unit_dict[\"ì¶”ì²œ_ëŒ€ìƒ\"] else \"ì •ë³´ ì—†ìŒ\"\n",
    "    \n",
    "    # ìš”ì²­í•˜ì‹  ëŒ€ë¡œ 'ì¹´ë“œì‚¬' ë¼ì¸ì„ ì¶”ê°€í•¨\n",
    "    return (\n",
    "        f\"# {unit_dict['name']}\\n\"\n",
    "        f\"ì¹´ë“œì‚¬ : {card_company}\\n\"\n",
    "        f\"ì—°íšŒë¹„ : {annual_fee}\\n\"\n",
    "        f\"OTT í˜œíƒ : {ott_combined}\\n\"\n",
    "        f\"ì¶”ì²œ ëŒ€ìƒ : {target}\"\n",
    "    )\n",
    "\n",
    "def generate_md_with_company():\n",
    "    # ì¶œë ¥ í´ë” ìƒì„±\n",
    "    os.makedirs(MD_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"ğŸ“‚ ì¹´ë“œì‚¬ ì •ë³´ê°€ í¬í•¨ëœ MD ë¬¸ì„œ ìƒì„± ì‹œì‘...\")\n",
    "\n",
    "    for card_name in card_list:\n",
    "        source_path = os.path.join(SOURCE_DIR, f\"{card_name}.docx\")\n",
    "        \n",
    "        if not os.path.exists(source_path):\n",
    "            print(f\"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê±´ë„ˆëœ€): {card_name}.docx\")\n",
    "            continue\n",
    "\n",
    "        doc = Document(source_path)\n",
    "        raw_lines = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "        \n",
    "        products_md = []\n",
    "        current_unit = None\n",
    "\n",
    "        for line in raw_lines:\n",
    "            # 1. ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°\n",
    "            line = re.sub(r'\\\\s*', '', line)\n",
    "            \n",
    "            # 2. ìƒí’ˆëª… ê°ì§€ (ìŠ¤í”Œë¦¿ ì§€ì )\n",
    "            header_match = re.search(r'\\[(.*?)\\]\\s*(.*)', line)\n",
    "            if header_match:\n",
    "                if current_unit:\n",
    "                    # finalize_unit í˜¸ì¶œ ì‹œ card_name(ì¹´ë“œì‚¬)ì„ ì¸ìë¡œ ì „ë‹¬\n",
    "                    products_md.append(finalize_unit(current_unit, card_name))\n",
    "                \n",
    "                p_name = header_match.group(2).strip()\n",
    "                current_unit = {\n",
    "                    \"name\": p_name,\n",
    "                    \"ì—°íšŒë¹„\": \"\",\n",
    "                    \"OTT_í˜œíƒ\": [],\n",
    "                    \"ì¶”ì²œ_ëŒ€ìƒ\": \"\",\n",
    "                    \"state\": \"normal\"\n",
    "                }\n",
    "            \n",
    "            elif current_unit:\n",
    "                # 3. ë°ì´í„° ë§¤ì¹­ ë¡œì§\n",
    "                if \"ì—°íšŒë¹„ :\" in line:\n",
    "                    current_unit[\"ì—°íšŒë¹„\"] = line.split(\":\", 1)[-1].strip()\n",
    "                    current_unit[\"state\"] = \"normal\"\n",
    "                elif \"OTT í˜œíƒ\" in line:\n",
    "                    current_unit[\"state\"] = \"ott\"\n",
    "                elif \"ì¶”ì²œ ëŒ€ìƒ :\" in line:\n",
    "                    current_unit[\"ì¶”ì²œ_ëŒ€ìƒ\"] = line.split(\":\", 1)[-1].strip()\n",
    "                    current_unit[\"state\"] = \"normal\"\n",
    "                elif current_unit[\"state\"] == \"ott\":\n",
    "                    clean_benefit = re.sub(r'^[lnÂ·-]\\s+', '', line)\n",
    "                    current_unit[\"OTT_í˜œíƒ\"].append(clean_benefit)\n",
    "\n",
    "        # ë§ˆì§€ë§‰ ìƒí’ˆ ì¶”ê°€\n",
    "        if current_unit:\n",
    "            products_md.append(finalize_unit(current_unit, card_name))\n",
    "\n",
    "        # 4. íŒŒì¼ ì €ì¥\n",
    "        if products_md:\n",
    "            md_file_path = os.path.join(MD_OUTPUT_DIR, f\"{card_name}.md\")\n",
    "            full_md_text = \"\\n\\n---\\n\\n\".join(products_md)\n",
    "            with open(md_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_md_text)\n",
    "            \n",
    "            print(f\"âœ… [{card_name}] MD ìƒì„± ì™„ë£Œ (ì¹´ë“œì‚¬ ì •ë³´ í¬í•¨)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_md_with_company()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae1a13",
   "metadata": {},
   "source": [
    "## ì„ë² ë”© ë° json íŒŒì¼ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22659bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  MD ê¸°ë°˜ ê°€ë³€ ì²­í‚¹ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...\n",
      "âœ… [ì‚¼ì„±ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 8ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [KBêµ­ë¯¼ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 8ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ë¡¯ë°ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 5ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ì‹ í•œì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 12ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [NHë†í˜‘ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 4ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [í•˜ë‚˜ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 3ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [IBKê¸°ì—…ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ìš°ë¦¬ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 3ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [BCë°”ë¡œì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 2ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [í˜„ëŒ€ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 2ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [iMë±…í¬] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ì¼€ì´ë±…í¬] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [BNKë¶€ì‚°ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ê´‘ì£¼ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ì œì£¼ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [í† ìŠ¤ë±…í¬] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ìš°ì²´êµ­] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ë¯¸ë˜ì—ì…‹ì¦ê¶Œ] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- ê²½ë¡œ ë° ì„¤ì • ---\n",
    "MD_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ì‹œë‚˜ë¦¬ì˜¤3ë²ˆìš©RAG\\MD_ê°œë³„íŒŒì¼\"\n",
    "JSON_OUTPUT_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\jsoní´ë”\\ê°œë³„íŒŒì¼\"\n",
    "\n",
    "# ì²˜ë¦¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸\n",
    "card_list = [\n",
    "    \"ì‚¼ì„±ì¹´ë“œ\", \"KBêµ­ë¯¼ì¹´ë“œ\", \"ë¡¯ë°ì¹´ë“œ\", \"ì‹ í•œì¹´ë“œ\", \"NHë†í˜‘ì¹´ë“œ\", \"í•˜ë‚˜ì¹´ë“œ\",\n",
    "    \"IBKê¸°ì—…ì€í–‰\", \"ìš°ë¦¬ì¹´ë“œ\", \"BCë°”ë¡œì¹´ë“œ\", \"í˜„ëŒ€ì¹´ë“œ\", \"iMë±…í¬\", \"ì¼€ì´ë±…í¬\",\n",
    "    \"BNKë¶€ì‚°ì€í–‰\", \"ê´‘ì£¼ì€í–‰\", \"ì œì£¼ì€í–‰\", \"í† ìŠ¤ë±…í¬\", \"ìš°ì²´êµ­\", \"ë¯¸ë˜ì—ì…‹ì¦ê¶Œ\"\n",
    "]\n",
    "\n",
    "# ì²­í¬ ìµœëŒ€ ê¸€ì ìˆ˜\n",
    "MAX_CHARS_PER_CHUNK = 800\n",
    "\n",
    "print(\"ğŸ§  MD ê¸°ë°˜ ê°€ë³€ ì²­í‚¹ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def run_flexible_chunking():\n",
    "    os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for card_name in card_list:\n",
    "        md_path = os.path.join(MD_DIR, f\"{card_name}.md\")\n",
    "        \n",
    "        if not os.path.exists(md_path):\n",
    "            print(f\"âš ï¸ MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {card_name}.md\")\n",
    "            continue\n",
    "\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # 1. '---' êµ¬ë¶„ì„ ìœ¼ë¡œ ê°œë³„ ìƒí’ˆ ìœ ë‹› ë¶„ë¦¬ \n",
    "        units = [u.strip() for u in content.split('---') if u.strip()]\n",
    "        \n",
    "        final_chunks = []\n",
    "        temp_chunk_text = \"\"\n",
    "        \n",
    "        # 2. ê¸€ì ìˆ˜(1000ì)ì— ë§ì¶˜ ë³‘í•© ë¡œì§\n",
    "        for unit in units:\n",
    "            # í˜„ì¬ ëˆ„ì  í…ìŠ¤íŠ¸ + ë‹¤ìŒ ìƒí’ˆ ìœ ë‹›ì´ 1000ìë¥¼ ë„˜ìœ¼ë©´ í˜„ì¬ê¹Œì§€ë¥¼ ì²­í¬ë¡œ í™•ì •\n",
    "            if len(temp_chunk_text) + len(unit) > MAX_CHARS_PER_CHUNK:\n",
    "                if temp_chunk_text:\n",
    "                    final_chunks.append(temp_chunk_text.strip())\n",
    "                temp_chunk_text = unit\n",
    "            else:\n",
    "                # 1000ì ë¯¸ë§Œì´ë©´ ê³„ì† ë³‘í•©\n",
    "                if temp_chunk_text:\n",
    "                    temp_chunk_text += \"\\n\\n---\\n\\n\" + unit\n",
    "                else:\n",
    "                    temp_chunk_text = unit\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ë‚¨ì€ í…ìŠ¤íŠ¸ ì¶”ê°€\n",
    "        if temp_chunk_text:\n",
    "            final_chunks.append(temp_chunk_text.strip())\n",
    "\n",
    "        # 3. ì²­í¬ë³„ ì„ë² ë”© ë° JSON ìƒì„±\n",
    "        card_json_results = []\n",
    "        for i, chunk_text in enumerate(final_chunks):\n",
    "            # ì²« ë²ˆì§¸ ìƒí’ˆëª…ì„ ì¶”ì¶œí•˜ì—¬ source í•„ë“œ ëŒ€í‘œê°’ìœ¼ë¡œ ì‚¬ìš© \n",
    "            header_match = re.search(r\"#\\s*(.+)\", chunk_text)\n",
    "            source_name = header_match.group(1).strip() if header_match else card_name\n",
    "\n",
    "            dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "            \n",
    "            card_json_results.append({\n",
    "                \"pk\": f\"{card_name}_CHUNK_{i+1:02d}\",\n",
    "                \"source\": card_name,\n",
    "                \"text\": chunk_text,\n",
    "                \"dense\": dense_vec.tolist(),\n",
    "                \"file_hash\": f\"{card_name}.md\",\n",
    "                \"page\": 1,\n",
    "                \"row\": 1\n",
    "            })\n",
    "\n",
    "        # 4. ì¹´ë“œì‚¬ë³„ JSON ì €ì¥\n",
    "        json_file_path = os.path.join(JSON_OUTPUT_DIR, f\"{card_name}.json\")\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(card_json_results, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"âœ… [{card_name}] ì²­í‚¹ ì™„ë£Œ: {len(final_chunks)}ê°œì˜ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_flexible_chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53888f78",
   "metadata": {},
   "source": [
    "## ê¸€ììˆ˜ 900ìë¡œ ì²­í‚¹ (ìµœì )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "682fe2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  MD ê¸°ë°˜ ê°€ë³€ ì²­í‚¹ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...\n",
      "âœ… [ì‚¼ì„±ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 6ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [KBêµ­ë¯¼ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 7ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ë¡¯ë°ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 4ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ì‹ í•œì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 11ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [NHë†í˜‘ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 4ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [í•˜ë‚˜ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 3ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [IBKê¸°ì—…ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ìš°ë¦¬ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 3ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [BCë°”ë¡œì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 2ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [í˜„ëŒ€ì¹´ë“œ] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [iMë±…í¬] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ì¼€ì´ë±…í¬] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [BNKë¶€ì‚°ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ê´‘ì£¼ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ì œì£¼ì€í–‰] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [í† ìŠ¤ë±…í¬] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ìš°ì²´êµ­] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n",
      "âœ… [ë¯¸ë˜ì—ì…‹ì¦ê¶Œ] ì²­í‚¹ ì™„ë£Œ: 1ê°œì˜ ì²­í¬ ìƒì„±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- ê²½ë¡œ ë° ì„¤ì • ---\n",
    "MD_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ì‹œë‚˜ë¦¬ì˜¤3ë²ˆìš©RAG\\MD_ê°œë³„íŒŒì¼\"\n",
    "JSON_OUTPUT_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\jsoní´ë”\\ì¹´ë“œ_ì€í–‰íŒŒì¼\"\n",
    "\n",
    "# ì²˜ë¦¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸\n",
    "card_list = [\n",
    "    \"ì‚¼ì„±ì¹´ë“œ\", \"KBêµ­ë¯¼ì¹´ë“œ\", \"ë¡¯ë°ì¹´ë“œ\", \"ì‹ í•œì¹´ë“œ\", \"NHë†í˜‘ì¹´ë“œ\", \"í•˜ë‚˜ì¹´ë“œ\",\n",
    "    \"IBKê¸°ì—…ì€í–‰\", \"ìš°ë¦¬ì¹´ë“œ\", \"BCë°”ë¡œì¹´ë“œ\", \"í˜„ëŒ€ì¹´ë“œ\", \"iMë±…í¬\", \"ì¼€ì´ë±…í¬\",\n",
    "    \"BNKë¶€ì‚°ì€í–‰\", \"ê´‘ì£¼ì€í–‰\", \"ì œì£¼ì€í–‰\", \"í† ìŠ¤ë±…í¬\", \"ìš°ì²´êµ­\", \"ë¯¸ë˜ì—ì…‹ì¦ê¶Œ\"\n",
    "]\n",
    "\n",
    "# ì²­í¬ ìµœëŒ€ ê¸€ì ìˆ˜\n",
    "MAX_CHARS_PER_CHUNK = 900\n",
    "\n",
    "print(\"ğŸ§  MD ê¸°ë°˜ ê°€ë³€ ì²­í‚¹ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...\")\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def run_flexible_chunking():\n",
    "    os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for card_name in card_list:\n",
    "        md_path = os.path.join(MD_DIR, f\"{card_name}.md\")\n",
    "        \n",
    "        if not os.path.exists(md_path):\n",
    "            print(f\"âš ï¸ MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {card_name}.md\")\n",
    "            continue\n",
    "\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # 1. '---' êµ¬ë¶„ì„ ìœ¼ë¡œ ê°œë³„ ìƒí’ˆ ìœ ë‹› ë¶„ë¦¬ \n",
    "        units = [u.strip() for u in content.split('---') if u.strip()]\n",
    "        \n",
    "        final_chunks = []\n",
    "        temp_chunk_text = \"\"\n",
    "        \n",
    "        # 2. ê¸€ì ìˆ˜(1000ì)ì— ë§ì¶˜ ë³‘í•© ë¡œì§\n",
    "        for unit in units:\n",
    "            # í˜„ì¬ ëˆ„ì  í…ìŠ¤íŠ¸ + ë‹¤ìŒ ìƒí’ˆ ìœ ë‹›ì´ 1000ìë¥¼ ë„˜ìœ¼ë©´ í˜„ì¬ê¹Œì§€ë¥¼ ì²­í¬ë¡œ í™•ì •\n",
    "            if len(temp_chunk_text) + len(unit) > MAX_CHARS_PER_CHUNK:\n",
    "                if temp_chunk_text:\n",
    "                    final_chunks.append(temp_chunk_text.strip())\n",
    "                temp_chunk_text = unit\n",
    "            else:\n",
    "                # 1000ì ë¯¸ë§Œì´ë©´ ê³„ì† ë³‘í•©\n",
    "                if temp_chunk_text:\n",
    "                    temp_chunk_text += \"\\n\\n---\\n\\n\" + unit\n",
    "                else:\n",
    "                    temp_chunk_text = unit\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ë‚¨ì€ í…ìŠ¤íŠ¸ ì¶”ê°€\n",
    "        if temp_chunk_text:\n",
    "            final_chunks.append(temp_chunk_text.strip())\n",
    "\n",
    "        # 3. ì²­í¬ë³„ ì„ë² ë”© ë° JSON ìƒì„±\n",
    "        card_json_results = []\n",
    "        for i, chunk_text in enumerate(final_chunks):\n",
    "            # ì²« ë²ˆì§¸ ìƒí’ˆëª…ì„ ì¶”ì¶œí•˜ì—¬ source í•„ë“œ ëŒ€í‘œê°’ìœ¼ë¡œ ì‚¬ìš© \n",
    "            header_match = re.search(r\"#\\s*(.+)\", chunk_text)\n",
    "            source_name = header_match.group(1).strip() if header_match else card_name\n",
    "\n",
    "            dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "            \n",
    "            card_json_results.append({\n",
    "                \"pk\": f\"{card_name}_CHUNK_{i+1:02d}\",\n",
    "                \"source\": card_name,\n",
    "                \"text\": chunk_text,\n",
    "                \"dense\": dense_vec.tolist(),\n",
    "                \"file_hash\": f\"{card_name}.md\",\n",
    "                \"page\": 1,\n",
    "                \"row\": 1\n",
    "            })\n",
    "\n",
    "        # 4. ì¹´ë“œì‚¬ë³„ JSON ì €ì¥\n",
    "        json_file_path = os.path.join(JSON_OUTPUT_DIR, f\"{card_name}.json\")\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(card_json_results, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"âœ… [{card_name}] ì²­í‚¹ ì™„ë£Œ: {len(final_chunks)}ê°œì˜ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_flexible_chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4d855",
   "metadata": {},
   "source": [
    "# êµ¬ë…ìƒí’ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0590766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ êµ¬ë… ìƒí’ˆ ì •ë³´ê°€ í¬í•¨ëœ MD ë¬¸ì„œ ìƒì„± ì‹œì‘...\n",
      "âœ… [KTêµ¬ë…] MD ìƒì„± ì™„ë£Œ\n",
      "âœ… [Tìš°ì£¼íŒ¨ìŠ¤] MD ìƒì„± ì™„ë£Œ\n",
      "âœ… [ë„¤ì´ë²„ë©¤ë²„ì‹­] MD ìƒì„± ì™„ë£Œ\n",
      "âœ… [ë°°ë¯¼í´ëŸ½] MD ìƒì„± ì™„ë£Œ\n",
      "âœ… [ìœ ë…] MD ìƒì„± ì™„ë£Œ\n",
      "âœ… [í‹°ë¹™] MD ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# --- ê²½ë¡œ ì„¤ì • ---\n",
    "# êµ¬ë… ë¬¸ì„œê°€ ìˆëŠ” ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "SOURCE_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ì‹œë‚˜ë¦¬ì˜¤3ë²ˆìš©RAG\\êµ¬ë…_0102\"\n",
    "MD_OUTPUT_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ìµœì¢…RAG\\êµ¬ë…ìƒí’ˆ\\md\"\n",
    "\n",
    "# êµ¬ë… ì„œë¹„ìŠ¤ ë¦¬ìŠ¤íŠ¸ (í´ë” ë‚´ íŒŒì¼ëª…ì— ë§ì¶° ì¶”ê°€)\n",
    "subscription_list = [\n",
    "    \"KTêµ¬ë…\", \"Tìš°ì£¼íŒ¨ìŠ¤\", \"ë„¤ì´ë²„ë©¤ë²„ì‹­\", \"ë°°ë¯¼í´ëŸ½\", \"ìœ ë…\", \"í‹°ë¹™\"\n",
    "]\n",
    "\n",
    "def finalize_unit(unit_dict, service_category):\n",
    "    ott_combined = \", \".join(unit_dict[\"OTT_í˜œíƒ\"]) if unit_dict[\"OTT_í˜œíƒ\"] else \"ì •ë³´ ì—†ìŒ\"\n",
    "    price_info = unit_dict[\"ì´ìš©ë£Œ\"] if unit_dict[\"ì´ìš©ë£Œ\"] else \"ì •ë³´ ì—†ìŒ\"\n",
    "    target = unit_dict[\"ì¶”ì²œ_ëŒ€ìƒ\"] if unit_dict[\"ì¶”ì²œ_ëŒ€ìƒ\"] else \"ì •ë³´ ì—†ìŒ\"\n",
    "    \n",
    "    return (\n",
    "        f\"# {unit_dict['name']}\\n\"\n",
    "        f\"ì„œë¹„ìŠ¤ : {service_category}\\n\"\n",
    "        f\"ì´ìš©ë£Œ : {price_info}\\n\"\n",
    "        f\"OTT í˜œíƒ : {ott_combined}\\n\"\n",
    "        f\"ì¶”ì²œ ëŒ€ìƒ : {target}\"\n",
    "    )\n",
    "\n",
    "def generate_subscription_md():\n",
    "    os.makedirs(MD_OUTPUT_DIR, exist_ok=True)\n",
    "    print(\"ğŸ“‚ êµ¬ë… ìƒí’ˆ ì •ë³´ê°€ í¬í•¨ëœ MD ë¬¸ì„œ ìƒì„± ì‹œì‘...\")\n",
    "\n",
    "    for service_name in subscription_list:\n",
    "        source_path = os.path.join(SOURCE_DIR, f\"{service_name}.docx\")\n",
    "        \n",
    "        if not os.path.exists(source_path):\n",
    "            print(f\"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {service_name}.docx\")\n",
    "            continue\n",
    "\n",
    "        doc = Document(source_path)\n",
    "        raw_lines = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "        \n",
    "        products_md = []\n",
    "        current_unit = None\n",
    "\n",
    "        for line in raw_lines:\n",
    "            # 1. íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "            line = re.sub(r'\\\\s*', '', line)\n",
    "            \n",
    "            # 2. ìƒí’ˆëª… ê°ì§€ ([KT êµ¬ë…] ìƒí’ˆëª… íŒ¨í„´)\n",
    "            header_match = re.search(r'\\[(.*?)\\]\\s*(.*)', line)\n",
    "            if header_match:\n",
    "                if current_unit:\n",
    "                    products_md.append(finalize_unit(current_unit, service_name))\n",
    "                \n",
    "                p_name = header_match.group(2).strip()\n",
    "                current_unit = {\n",
    "                    \"name\": p_name,\n",
    "                    \"ì´ìš©ë£Œ\": \"\",\n",
    "                    \"OTT_í˜œíƒ\": [],\n",
    "                    \"ì¶”ì²œ_ëŒ€ìƒ\": \"\",\n",
    "                    \"state\": \"normal\"\n",
    "                }\n",
    "            \n",
    "            elif current_unit:\n",
    "                # 3. ë°ì´í„° ë§¤ì¹­ (êµ¬ë… ì „ìš© í‚¤ì›Œë“œ: ì´ìš©ë£Œ, OTT í˜œíƒ, ì¶”ì²œ ëŒ€ìƒ)\n",
    "                if \"ì´ìš©ë£Œ :\" in line:\n",
    "                    current_unit[\"ì´ìš©ë£Œ\"] = line.split(\":\", 1)[-1].strip()\n",
    "                    current_unit[\"state\"] = \"normal\"\n",
    "                elif \"OTT í˜œíƒ\" in line:\n",
    "                    current_unit[\"state\"] = \"ott\"\n",
    "                elif \"ì¶”ì²œ ëŒ€ìƒ :\" in line:\n",
    "                    current_unit[\"ì¶”ì²œ_ëŒ€ìƒ\"] = line.split(\":\", 1)[-1].strip()\n",
    "                    current_unit[\"state\"] = \"normal\"\n",
    "                elif current_unit[\"state\"] == \"ott\":\n",
    "                    # ìƒì„¸ í˜œíƒ ìˆ˜ì§‘ (ìš”ê¸ˆì œ ì •ë³´ ë“±)\n",
    "                    clean_benefit = re.sub(r'^[lnÂ·-]\\s+', '', line)\n",
    "                    current_unit[\"OTT_í˜œíƒ\"].append(clean_benefit)\n",
    "\n",
    "        if current_unit:\n",
    "            products_md.append(finalize_unit(current_unit, service_name))\n",
    "\n",
    "        if products_md:\n",
    "            md_file_path = os.path.join(MD_OUTPUT_DIR, f\"{service_name}.md\")\n",
    "            full_md_text = \"\\n\\n---\\n\\n\".join(products_md)\n",
    "            with open(md_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_md_text)\n",
    "            \n",
    "            print(f\"âœ… [{service_name}] MD ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_subscription_md()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24834b0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# --- ê²½ë¡œ ë° ì„¤ì • ---\u001b[39;00m\n\u001b[0;32m      8\u001b[0m MD_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124më¯¸ì†Œì •ë³´ê¸°ìˆ \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - ì¸í•˜ëŒ€í•™êµ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124më°”íƒ• í™”ë©´\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRAGíŒŒì¼\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mìµœì¢…RAG\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mêµ¬ë…ìƒí’ˆ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\sentence_transformers\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     CrossEncoder,\n\u001b[0;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\sentence_transformers\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\sentence_transformers\\backend\\load.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\transformers\\__init__.py:958\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m    956\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 958\u001b[0m import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})]\u001b[38;5;241m.\u001b[39mupdate(_import_structure)\n\u001b[0;32m    961\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m     extra_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__},\n\u001b[0;32m    967\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2867\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[1;34m(module_path, prefix)\u001b[0m\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[0;32m   2847\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;124;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2867\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2868\u001b[0m     spread_dict \u001b[38;5;241m=\u001b[39m spread_import_structure(import_structure)\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2580\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(module_path):\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pycache__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f)):\n\u001b[1;32m-> 2580\u001b[0m         import_structure[f] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2582\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f)):\n\u001b[0;32m   2583\u001b[0m         adjacent_modules\u001b[38;5;241m.\u001b[39mappend(f)\n",
      "File \u001b[1;32mc:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:2604\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2602\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   2605\u001b[0m     file_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- ê²½ë¡œ ë° ì„¤ì • ---\n",
    "MD_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAGíŒŒì¼\\ìµœì¢…RAG\\êµ¬ë…ìƒí’ˆ\\md\"\n",
    "JSON_OUTPUT_DIR = r\"C:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\OneDrive - ì¸í•˜ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\RAG\\jsoní´ë”\\êµ¬ë…ìƒí’ˆ\"\n",
    "\n",
    "# ì²˜ë¦¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ (ë³€ìˆ˜ëª… í†µì¼)\n",
    "subscription_list = [\n",
    "    \"KTêµ¬ë…\", \"Tìš°ì£¼íŒ¨ìŠ¤\", \"ë„¤ì´ë²„ë©¤ë²„ì‹­\", \"ë°°ë¯¼í´ëŸ½\", \"ìœ ë…\", \"í‹°ë¹™\"\n",
    "]\n",
    "\n",
    "# ì²­í¬ ìµœëŒ€ ê¸€ì ìˆ˜\n",
    "MAX_CHARS_PER_CHUNK = 1200\n",
    "\n",
    "print(\"ğŸ§  êµ¬ë… ìƒí’ˆ MD ê¸°ë°˜ ê°€ë³€ ì²­í‚¹ ë° ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v2-moe\", trust_remote_code=True)\n",
    "\n",
    "def run_flexible_chunking():\n",
    "    os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    for service_name in subscription_list:\n",
    "        md_path = os.path.join(MD_DIR, f\"{service_name}.md\")\n",
    "        \n",
    "        if not os.path.exists(md_path):\n",
    "            print(f\"âš ï¸ MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {md_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # 1. '---' êµ¬ë¶„ì„ ìœ¼ë¡œ ê°œë³„ ìƒí’ˆ ìœ ë‹› ë¶„ë¦¬ \n",
    "        units = [u.strip() for u in content.split('---') if u.strip()]\n",
    "        \n",
    "        final_chunks = []\n",
    "        temp_chunk_text = \"\"\n",
    "        \n",
    "        # 2. ê°€ë³€ ì²­í‚¹ ë¡œì§ (900ì ê¸°ì¤€ ë³‘í•©)\n",
    "        for unit in units:\n",
    "            # ë‹¤ìŒ ìœ ë‹›ì„ í•©ì³¤ì„ ë•Œ 900ìë¥¼ ë„˜ìœ¼ë©´ í˜„ì¬ê¹Œì§€ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìƒì„±\n",
    "            if len(temp_chunk_text) + len(unit) > MAX_CHARS_PER_CHUNK:\n",
    "                if temp_chunk_text:\n",
    "                    final_chunks.append(temp_chunk_text.strip())\n",
    "                temp_chunk_text = unit\n",
    "            else:\n",
    "                # 900ì ë¯¸ë§Œì´ë©´ '---' êµ¬ë¶„ì„ ê³¼ í•¨ê»˜ ë³‘í•©\n",
    "                if temp_chunk_text:\n",
    "                    temp_chunk_text += \"\\n\\n----- \\n\\n\" + unit\n",
    "                else:\n",
    "                    temp_chunk_text = unit\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "        if temp_chunk_text:\n",
    "            final_chunks.append(temp_chunk_text.strip())\n",
    "\n",
    "        # 3. ì²­í¬ë³„ ì„ë² ë”© ìƒì„± ë° JSON êµ¬ì„±\n",
    "        service_results = []\n",
    "        for i, chunk_text in enumerate(final_chunks):\n",
    "            # --- gRPC Rate Limit(0.1) ëŒ€ì‘: ìš”ì²­ ê°„ 1ì´ˆ ëŒ€ê¸° ---\n",
    "            time.sleep(1.0)\n",
    "            \n",
    "            try:\n",
    "                # ì„ë² ë”© ìƒì„±\n",
    "                dense_vec = model.encode([chunk_text], normalize_embeddings=True)[0]\n",
    "                \n",
    "                service_results.append({\n",
    "                    \"pk\": f\"{service_name}_CHUNK_{i+1:02d}\",\n",
    "                    \"source\": service_name,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"dense\": dense_vec.tolist(),\n",
    "                    \"file_hash\": f\"{service_name}.md\",\n",
    "                    \"page\": 1,\n",
    "                    \"row\": 1\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {service_name} ì²­í¬ {i+1} ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "                time.sleep(10) # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ê¸° ì‹œê°„ ì—°ì¥\n",
    "                continue\n",
    "\n",
    "        # 4. ì„œë¹„ìŠ¤ë³„ ê°œë³„ JSON ì €ì¥\n",
    "        json_file_path = os.path.join(JSON_OUTPUT_DIR, f\"{service_name}.json\")\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(service_results, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"âœ… [{service_name}] ì™„ë£Œ: {len(final_chunks)}ê°œì˜ ì²­í¬ê°€ {json_file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_flexible_chunking()\n",
    "    print(\"\\nğŸš€ ëª¨ë“  êµ¬ë… ìƒí’ˆì˜ ì²­í‚¹ ë° ì„ë² ë”© ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
